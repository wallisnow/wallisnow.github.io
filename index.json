[{"content":"需求 这两天我需要kubernetes 上面没有ready 的pod列出来, 然后对这些pod进行操作, 又要用到ansible, 思来想去不如来个map, 那么 python 或者ansible里面叫dict\n输入 这里有一个 kubectl get pods 得出的结果, 使用ansible的 shell, 我们需要处理其中的stdout, 或者stdout_lines\nchanged: [localhost -\u0026gt; fd00:eccd:0:a0a::5] =\u0026gt; { \u0026quot;ansible_facts\u0026quot;: { \u0026quot;discovered_interpreter_python\u0026quot;: \u0026quot;/usr/bin/python\u0026quot; }, \u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;/usr/local/bin/kubectl get pods -n kube-system --field-selector spec.nodeName=poolalpha-worker-0-efggjjp-ansibd-01 -o custom-columns=NAME:.metadata.name,\\\u0026quot;IS_READY\\\u0026quot;:.status.containerStatuses[].ready --no-headers\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:00.475861\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2021-07-29 19:37:41.515968\u0026quot;, \u0026quot;invocation\u0026quot;: { \u0026quot;module_args\u0026quot;: { \u0026quot;_raw_params\u0026quot;: \u0026quot;/usr/local/bin/kubectl get pods -n kube-system --field-selector spec.nodeName=poolalpha-worker-0-efggjjp-ansibd-01 -o custom-columns=NAME:.metadata.name,\\\u0026quot;IS_READY\\\u0026quot;:.status.containerStatuses[].ready --no-headers\u0026quot;, \u0026quot;_uses_shell\u0026quot;: true, \u0026quot;argv\u0026quot;: null, \u0026quot;chdir\u0026quot;: null, \u0026quot;creates\u0026quot;: null, \u0026quot;executable\u0026quot;: null, \u0026quot;removes\u0026quot;: null, \u0026quot;stdin\u0026quot;: null, \u0026quot;stdin_add_newline\u0026quot;: true, \u0026quot;strip_empty_ends\u0026quot;: true, \u0026quot;warn\u0026quot;: true } }, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2021-07-29 19:37:41.040107\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr_lines\u0026quot;: [], ... \u0026quot;stdout_lines\u0026quot;: [ \u0026quot;nginx-deployment-66b6c48dd5-4crpt true\u0026quot;, \u0026quot;nginx-deployment-66b6c48dd5-hqlxm false\u0026quot; ] } 需要的数据结构 我需要得到一个dict, pod_status, 然后key 是pod名, value是pod(容器)状态, 也就是这样滴-\u0026gt;\n\u0026quot;pod_status\u0026quot;: { \u0026quot;nginx-deployment-66b6c48dd5-4crpt\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;nginx-deployment-66b6c48dd5-hqlxm\u0026quot;: \u0026quot;false\u0026quot; } 具体实现 --- - name: test key value list to dict hosts: localhost gather_facts: false tasks: - name: set values set_fact: status_list: - \u0026quot;nginx-deployment-66b6c48dd5-4crpt true\u0026quot; - \u0026quot;nginx-deployment-66b6c48dd5-hqlxm false\u0026quot; - name: Set pod-status dict vars: pod: \u0026quot;{{ item.split()[0]|trim }}\u0026quot; status: \u0026quot;{{ item.split()[1]|trim }}\u0026quot; set_fact: pod_status: \u0026quot;{{ pod_status | default({}) | combine({pod : status})}}\u0026quot; loop: \u0026quot;{{ status_list }}\u0026quot;   这里使用了combine 函数+var 配合的方法, loop循环status_list, 所以会迭代两个item\n \u0026ldquo;nginx-deployment-66b6c48dd5-4crpt true\u0026rdquo; \u0026ldquo;nginx-deployment-66b6c48dd5-4crpt true\u0026rdquo;    这时再使用.split()来切割字符串, 那么其就会变成一个长度为 2 的数组, 也就是 item.split()[0] 和 item.split()[1], 里面分别装pod名和状态.\n  然后他们会被赋值到vars\n  然后通过combine函数生成数据结构为{pod : status} 的map, 也就是dictionary, 因为combine是merge的规则, 所以迭代后会叠加下去, 也就生成了\n  \u0026quot;pod_status\u0026quot;: { \u0026quot;nginx-deployment-66b6c48dd5-4crpt\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;nginx-deployment-66b6c48dd5-hqlxm\u0026quot;: \u0026quot;false\u0026quot; } 赠瓶肥宅快乐水吧 ","permalink":"https://unprobug.com/post/ansible/key_value_list_to_dict/","summary":"需求 这两天我需要kubernetes 上面没有ready 的pod列出来, 然后对这些pod进行操作, 又要用到ansible, 思来想去不如来个map, 那么 python 或者ansible里面叫dict\n输入 这里有一个 kubectl get pods 得出的结果, 使用ansible的 shell, 我们需要处理其中的stdout, 或者stdout_lines\nchanged: [localhost -\u0026gt; fd00:eccd:0:a0a::5] =\u0026gt; { \u0026quot;ansible_facts\u0026quot;: { \u0026quot;discovered_interpreter_python\u0026quot;: \u0026quot;/usr/bin/python\u0026quot; }, \u0026quot;changed\u0026quot;: true, \u0026quot;cmd\u0026quot;: \u0026quot;/usr/local/bin/kubectl get pods -n kube-system --field-selector spec.nodeName=poolalpha-worker-0-efggjjp-ansibd-01 -o custom-columns=NAME:.metadata.name,\\\u0026quot;IS_READY\\\u0026quot;:.status.containerStatuses[].ready --no-headers\u0026quot;, \u0026quot;delta\u0026quot;: \u0026quot;0:00:00.475861\u0026quot;, \u0026quot;end\u0026quot;: \u0026quot;2021-07-29 19:37:41.515968\u0026quot;, \u0026quot;invocation\u0026quot;: { \u0026quot;module_args\u0026quot;: { \u0026quot;_raw_params\u0026quot;: \u0026quot;/usr/local/bin/kubectl get pods -n kube-system --field-selector spec.nodeName=poolalpha-worker-0-efggjjp-ansibd-01 -o custom-columns=NAME:.metadata.name,\\\u0026quot;IS_READY\\\u0026quot;:.status.containerStatuses[].ready --no-headers\u0026quot;, \u0026quot;_uses_shell\u0026quot;: true, \u0026quot;argv\u0026quot;: null, \u0026quot;chdir\u0026quot;: null, \u0026quot;creates\u0026quot;: null, \u0026quot;executable\u0026quot;: null, \u0026quot;removes\u0026quot;: null, \u0026quot;stdin\u0026quot;: null, \u0026quot;stdin_add_newline\u0026quot;: true, \u0026quot;strip_empty_ends\u0026quot;: true, \u0026quot;warn\u0026quot;: true } }, \u0026quot;rc\u0026quot;: 0, \u0026quot;start\u0026quot;: \u0026quot;2021-07-29 19:37:41.","title":"[ansible]key value list 转 dictionary"},{"content":"0. 前言 有时因为一些原因, 我们需要修改kubelet 的配置, 新版本的kubernetes就是修改 config.yml, 那么这里有一个问题, 你怎么知道你的kubelet 已经生效了?\n1. 先说结论 查询方法为先开启kube proxy然后使用 kubernetes 提供的查询接口\napi/vi/nodes/\u0026lt;node_name\u0026gt;/proxy/cofigz 2. 举个栗子 例如我们有这样一个集群:\ntest@node-01:~\u0026gt; kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master-0-01 Ready control-plane,master 16h v1.21.1 10.0.10.9 \u0026lt;none\u0026gt; SUSE Linux Enterprise Server 15 SP2 5.3.18-24.67-default containerd://1.4.4 master-1-01 Ready control-plane,master 16h v1.21.1 10.0.10.10 \u0026lt;none\u0026gt; SUSE Linux Enterprise Server 15 SP2 5.3.18-24.67-default containerd://1.4.4 master-2-01 Ready control-plane,master 16h v1.21.1 10.0.10.24 \u0026lt;none\u0026gt; SUSE Linux Enterprise Server 15 SP2 5.3.18-24.67-default containerd://1.4.4 poolalpha-worker-0-01 Ready worker 15h v1.21.1 10.0.10.4 \u0026lt;none\u0026gt; SUSE Linux Enterprise Server 15 SP2 5.3.18-24.67-default containerd://1.4.4 poolalpha-worker-1-01 Ready worker 15h v1.21.1 10.0.10.13 \u0026lt;none\u0026gt; SUSE Linux Enterprise Server 15 SP2 5.3.18-24.67-default containerd://1.4.4 2.1 修改kubelet配置 接下来, 我们希望修改poolalpha-worker-0-01 的kubelet某个参数, 那么我们先登录这个节点, 然后查看当前节点使用的config.yml\ntest@poolalpha-worker-0-01:~\u0026gt; systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/local/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2021-07-26 15:44:19 UTC; 15h ago Docs: http://kubernetes.io/docs/ Process: 8868 ExecStopPost=/bin/umount --verbose /opt/cni (code=exited, status=0/SUCCESS) Process: 8874 ExecStartPre=/bin/mount --verbose --bind /usr/local/lib/cni /opt/cni (code=exited, status=0/SUCCESS) Process: 8869 ExecStartPre=/bin/mkdir --verbose --parents /opt/cni (code=exited, status=0/SUCCESS) Main PID: 8878 (kubelet) Tasks: 16 CGroup: /system.slice/kubelet.service └─8878 /usr/local/bin/kubelet ##### 这里可以看到kubelet使用的参数文件 --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network-plugin=cn\u0026gt; 我们观察到, 此时这个节点使用的配置文件为 \u0026ndash;config=/var/lib/kubelet/config.yaml, 打开看一下\ntest@poolalpha-worker-0-01:~\u0026gt; cat /var/lib/kubelet/config.yaml apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.crt authorization: mode: Webhook webhook: cacheAuthorizedTTL: 0s cacheUnauthorizedTTL: 0s cgroupDriver: systemd clusterDNS: - 169.254.20.10 clusterDomain: cluster.local cpuManagerReconcilePeriod: 0s evictionPressureTransitionPeriod: 0s featureGates: AllAlpha: false BoundServiceAccountTokenVolume: false EphemeralContainers: true RemoveSelfLink: false RotateKubeletServerCertificate: true fileCheckFrequency: 0s healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 0s imageMinimumGCAge: 0s kind: KubeletConfiguration kubeletCgroups: /system.slice/kubelet.service logging: {} nodeStatusReportFrequency: 0s nodeStatusUpdateFrequency: 0s rotateCertificates: true runtimeRequestTimeout: 0s shutdownGracePeriod: 0s shutdownGracePeriodCriticalPods: 0s staticPodPath: /etc/kubernetes/manifests streamingConnectionIdleTimeout: 0s syncFrequency: 0s volumeStatsAggPeriod: 0s 此时, 我们修改优雅关机的相关参数,比如:\nshutdownGracePeriod: 30s shutdownGracePeriodCriticalPods: 10s 在此节点上重启kubelet\nsudo systemctl daemon-reload sudo systemctl stop kubelet sudo systemctl start kubelet 此时我们修改就完成了\n2.1 查看新配置是否生效 登录到一个有kubenetes 集群访问权限的节点, 比如一个master节点, 然后开启kube proxy\ntest@master-0-01:~\u0026gt; kubectl proxy --api-prefix=/ \u0026amp; [1] 6171 使用查询接口来查看\ntest@director-0-01:~\u0026gt; curl http://127.0.0.1:8001/api/v1/nodes/poolalpha-worker-0-01/proxy/configz|jq % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 2520 0 2520 0 0 7522 0 --:--:-- --:--:-- --:--:-- 7522 { \u0026quot;kubeletconfig\u0026quot;: { \u0026quot;enableServer\u0026quot;: true, \u0026quot;staticPodPath\u0026quot;: \u0026quot;/etc/kubernetes/manifests\u0026quot;, \u0026quot;syncFrequency\u0026quot;: \u0026quot;1m0s\u0026quot;, \u0026quot;fileCheckFrequency\u0026quot;: \u0026quot;20s\u0026quot;, \u0026quot;httpCheckFrequency\u0026quot;: \u0026quot;20s\u0026quot;, \u0026quot;address\u0026quot;: \u0026quot;0.0.0.0\u0026quot;, \u0026quot;port\u0026quot;: 10250, \u0026quot;tlsCertFile\u0026quot;: \u0026quot;/var/lib/kubelet/pki/kubelet.crt\u0026quot;, \u0026quot;tlsPrivateKeyFile\u0026quot;: \u0026quot;/var/lib/kubelet/pki/kubelet.key\u0026quot;, \u0026quot;rotateCertificates\u0026quot;: true, \u0026quot;authentication\u0026quot;: { \u0026quot;x509\u0026quot;: { \u0026quot;clientCAFile\u0026quot;: \u0026quot;/etc/kubernetes/pki/ca.crt\u0026quot; }, \u0026quot;webhook\u0026quot;: { \u0026quot;enabled\u0026quot;: true, \u0026quot;cacheTTL\u0026quot;: \u0026quot;2m0s\u0026quot; }, \u0026quot;anonymous\u0026quot;: { \u0026quot;enabled\u0026quot;: false } }, \u0026quot;authorization\u0026quot;: { \u0026quot;mode\u0026quot;: \u0026quot;Webhook\u0026quot;, \u0026quot;webhook\u0026quot;: { \u0026quot;cacheAuthorizedTTL\u0026quot;: \u0026quot;5m0s\u0026quot;, \u0026quot;cacheUnauthorizedTTL\u0026quot;: \u0026quot;30s\u0026quot; } }, \u0026quot;registryPullQPS\u0026quot;: 5, \u0026quot;registryBurst\u0026quot;: 10, \u0026quot;eventRecordQPS\u0026quot;: 5, \u0026quot;eventBurst\u0026quot;: 10, \u0026quot;enableDebuggingHandlers\u0026quot;: true, \u0026quot;healthzPort\u0026quot;: 10248, \u0026quot;healthzBindAddress\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, \u0026quot;oomScoreAdj\u0026quot;: -999, \u0026quot;clusterDomain\u0026quot;: \u0026quot;cluster.local\u0026quot;, \u0026quot;clusterDNS\u0026quot;: [ \u0026quot;169.254.20.10\u0026quot; ], \u0026quot;streamingConnectionIdleTimeout\u0026quot;: \u0026quot;4h0m0s\u0026quot;, \u0026quot;nodeStatusUpdateFrequency\u0026quot;: \u0026quot;10s\u0026quot;, \u0026quot;nodeStatusReportFrequency\u0026quot;: \u0026quot;5m0s\u0026quot;, \u0026quot;nodeLeaseDurationSeconds\u0026quot;: 40, \u0026quot;imageMinimumGCAge\u0026quot;: \u0026quot;2m0s\u0026quot;, \u0026quot;imageGCHighThresholdPercent\u0026quot;: 85, \u0026quot;imageGCLowThresholdPercent\u0026quot;: 80, \u0026quot;volumeStatsAggPeriod\u0026quot;: \u0026quot;1m0s\u0026quot;, \u0026quot;kubeletCgroups\u0026quot;: \u0026quot;/system.slice/kubelet.service\u0026quot;, \u0026quot;cgroupsPerQOS\u0026quot;: true, \u0026quot;cgroupDriver\u0026quot;: \u0026quot;systemd\u0026quot;, \u0026quot;cpuManagerPolicy\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;cpuManagerReconcilePeriod\u0026quot;: \u0026quot;10s\u0026quot;, \u0026quot;memoryManagerPolicy\u0026quot;: \u0026quot;None\u0026quot;, \u0026quot;topologyManagerPolicy\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;topologyManagerScope\u0026quot;: \u0026quot;container\u0026quot;, \u0026quot;runtimeRequestTimeout\u0026quot;: \u0026quot;15m0s\u0026quot;, \u0026quot;hairpinMode\u0026quot;: \u0026quot;promiscuous-bridge\u0026quot;, \u0026quot;maxPods\u0026quot;: 110, \u0026quot;podPidsLimit\u0026quot;: -1, \u0026quot;resolvConf\u0026quot;: \u0026quot;/etc/resolv.conf\u0026quot;, \u0026quot;cpuCFSQuota\u0026quot;: true, \u0026quot;cpuCFSQuotaPeriod\u0026quot;: \u0026quot;100ms\u0026quot;, \u0026quot;nodeStatusMaxImages\u0026quot;: 50, \u0026quot;maxOpenFiles\u0026quot;: 1000000, \u0026quot;contentType\u0026quot;: \u0026quot;application/vnd.kubernetes.protobuf\u0026quot;, \u0026quot;kubeAPIQPS\u0026quot;: 5, \u0026quot;kubeAPIBurst\u0026quot;: 10, \u0026quot;serializeImagePulls\u0026quot;: true, \u0026quot;evictionHard\u0026quot;: { \u0026quot;imagefs.available\u0026quot;: \u0026quot;15%\u0026quot;, \u0026quot;memory.available\u0026quot;: \u0026quot;100Mi\u0026quot;, \u0026quot;nodefs.available\u0026quot;: \u0026quot;10%\u0026quot;, \u0026quot;nodefs.inodesFree\u0026quot;: \u0026quot;5%\u0026quot; }, \u0026quot;evictionPressureTransitionPeriod\u0026quot;: \u0026quot;5m0s\u0026quot;, \u0026quot;enableControllerAttachDetach\u0026quot;: true, \u0026quot;makeIPTablesUtilChains\u0026quot;: true, \u0026quot;iptablesMasqueradeBit\u0026quot;: 14, \u0026quot;iptablesDropBit\u0026quot;: 15, \u0026quot;featureGates\u0026quot;: { \u0026quot;AllAlpha\u0026quot;: false, \u0026quot;BoundServiceAccountTokenVolume\u0026quot;: false, \u0026quot;EphemeralContainers\u0026quot;: true, \u0026quot;RemoveSelfLink\u0026quot;: false, \u0026quot;RotateKubeletServerCertificate\u0026quot;: true }, \u0026quot;failSwapOn\u0026quot;: true, \u0026quot;containerLogMaxSize\u0026quot;: \u0026quot;10Mi\u0026quot;, \u0026quot;containerLogMaxFiles\u0026quot;: 5, \u0026quot;configMapAndSecretChangeDetectionStrategy\u0026quot;: \u0026quot;Watch\u0026quot;, \u0026quot;enforceNodeAllocatable\u0026quot;: [ \u0026quot;pods\u0026quot; ], \u0026quot;volumePluginDir\u0026quot;: \u0026quot;/usr/libexec/kubernetes/kubelet-plugins/volume/exec/\u0026quot;, \u0026quot;logging\u0026quot;: { \u0026quot;format\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;enableSystemLogHandler\u0026quot;: true, ################看这里#################### \u0026quot;shutdownGracePeriod\u0026quot;: \u0026quot;30s\u0026quot;, \u0026quot;shutdownGracePeriodCriticalPods\u0026quot;: \u0026quot;10s\u0026quot;, ######################################## \u0026quot;enableProfilingHandler\u0026quot;: true, \u0026quot;enableDebugFlagsHandler\u0026quot;: true } } 这里使用jq 使得生成的json更加可读\n3. 结语 上面修改kubelet参数只是一个例子, 修改的方法很多, 但查看其实际配置建议还是采用此接口\n赠瓶肥宅快乐水吧 ","permalink":"https://unprobug.com/post/kubernetes/how_to_check_kubelet_of_node/","summary":"0. 前言 有时因为一些原因, 我们需要修改kubelet 的配置, 新版本的kubernetes就是修改 config.yml, 那么这里有一个问题, 你怎么知道你的kubelet 已经生效了?\n1. 先说结论 查询方法为先开启kube proxy然后使用 kubernetes 提供的查询接口\napi/vi/nodes/\u0026lt;node_name\u0026gt;/proxy/cofigz 2. 举个栗子 例如我们有这样一个集群:\ntest@node-01:~\u0026gt; kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master-0-01 Ready control-plane,master 16h v1.21.1 10.0.10.9 \u0026lt;none\u0026gt; SUSE Linux Enterprise Server 15 SP2 5.3.18-24.67-default containerd://1.4.4 master-1-01 Ready control-plane,master 16h v1.21.1 10.0.10.10 \u0026lt;none\u0026gt; SUSE Linux Enterprise Server 15 SP2 5.3.18-24.67-default containerd://1.4.4 master-2-01 Ready control-plane,master 16h v1.","title":"[kubernetes]如何查看node的kubelet配置"},{"content":"1. 前置条件 准备一台centos 8 的机器, 我这里用的是vagrant安装的centos8镜像\n2. 安装过程 2.1 测试是否链接外网 $ ping google.com PING google.com (216.58.211.14) 56(84) bytes of data. 64 bytes from muc03s13-in-f14.1e100.net (216.58.211.14): icmp_seq=1 ttl=63 time=11.5 ms 64 bytes from muc03s13-in-f14.1e100.net (216.58.211.14): icmp_seq=2 ttl=63 time=12.1 ms ^C --- google.com ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 2ms rtt min/avg/max/mdev = 11.476/11.781/12.086/0.305 ms 2.2 删除旧的docker $ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 2.3 安装需要的依赖包 $ sudo yum install -y yum-utils //OUTPUT CentOS Linux 8 - AppStream 7.4 MB/s | 8.1 MB 00:01 CentOS Linux 8 - BaseOS 4.7 MB/s | 3.6 MB 00:00 CentOS Linux 8 - Extras 38 kB/s | 9.8 kB 00:00 Package yum-utils-4.0.17-5.el8.noarch is already installed. Dependencies resolved. ========================================================================================================================================================================================= Package Architecture Version Repository Size ========================================================================================================================================================================================= Upgrading: dnf-plugins-core noarch 4.0.18-4.el8 baseos 69 k python3-dnf-plugins-core noarch 4.0.18-4.el8 baseos 234 k yum-utils noarch 4.0.18-4.el8 baseos 71 k Transaction Summary ========================================================================================================================================================================================= Upgrade 3 Packages Total download size: 375 k Downloading Packages: (1/3): yum-utils-4.0.18-4.el8.noarch.rpm 1.3 MB/s | 71 kB 00:00 (2/3): dnf-plugins-core-4.0.18-4.el8.noarch.rpm 1.2 MB/s | 69 kB 00:00 (3/3): python3-dnf-plugins-core-4.0.18-4.el8.noarch.rpm 3.5 MB/s | 234 kB 00:00 ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Total 858 kB/s | 375 kB 00:00 warning: /var/cache/dnf/baseos-31c79d9833c65cf7/packages/dnf-plugins-core-4.0.18-4.el8.noarch.rpm: Header V3 RSA/SHA256 Signature, key ID 8483c65d: NOKEY CentOS Linux 8 - BaseOS 1.6 MB/s | 1.6 kB 00:00 Importing GPG key 0x8483C65D: Userid : \u0026quot;CentOS (CentOS Official Signing Key) \u0026lt;security@centos.org\u0026gt;\u0026quot; Fingerprint: 99DB 70FA E1D7 CE22 7FB6 4882 05B5 55B3 8483 C65D From : /etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial Key imported successfully Running transaction check Transaction check succeeded. Running transaction test Transaction test succeeded. Running transaction Preparing : 1/1 Upgrading : python3-dnf-plugins-core-4.0.18-4.el8.noarch 1/6 Upgrading : dnf-plugins-core-4.0.18-4.el8.noarch 2/6 Upgrading : yum-utils-4.0.18-4.el8.noarch 3/6 Cleanup : yum-utils-4.0.17-5.el8.noarch 4/6 Cleanup : dnf-plugins-core-4.0.17-5.el8.noarch 5/6 Cleanup : python3-dnf-plugins-core-4.0.17-5.el8.noarch 6/6 Running scriptlet: python3-dnf-plugins-core-4.0.17-5.el8.noarch 6/6 Verifying : dnf-plugins-core-4.0.18-4.el8.noarch 1/6 Verifying : dnf-plugins-core-4.0.17-5.el8.noarch 2/6 Verifying : python3-dnf-plugins-core-4.0.18-4.el8.noarch 3/6 Verifying : python3-dnf-plugins-core-4.0.17-5.el8.noarch 4/6 Verifying : yum-utils-4.0.18-4.el8.noarch 5/6 Verifying : yum-utils-4.0.17-5.el8.noarch 6/6 Upgraded: dnf-plugins-core-4.0.18-4.el8.noarch python3-dnf-plugins-core-4.0.18-4.el8.noarch yum-utils-4.0.18-4.el8.noarch Complete! 2.4 添加docker repo $ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 2.5 安装docker $ sudo yum install docker-ce docker-ce-cli containerd.io 3. 验证 重启docker\n$ sudo systemctl start docker 验证\n[vagrant@localhost ~]$ sudo docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world b8dfde127a29: Pull complete Digest: sha256:df5f5184104426b65967e016ff2ac0bfcd44ad7899ca3bbcf8e44e4461491a9e Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ 4. Ref: https://docs.docker.com/engine/install/centos/\n赠瓶肥宅快乐水吧 ","permalink":"https://unprobug.com/post/docker/how_to_install_docker_in_centos8/","summary":"1. 前置条件 准备一台centos 8 的机器, 我这里用的是vagrant安装的centos8镜像\n2. 安装过程 2.1 测试是否链接外网 $ ping google.com PING google.com (216.58.211.14) 56(84) bytes of data. 64 bytes from muc03s13-in-f14.1e100.net (216.58.211.14): icmp_seq=1 ttl=63 time=11.5 ms 64 bytes from muc03s13-in-f14.1e100.net (216.58.211.14): icmp_seq=2 ttl=63 time=12.1 ms ^C --- google.com ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 2ms rtt min/avg/max/mdev = 11.476/11.781/12.086/0.305 ms 2.2 删除旧的docker $ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 2.","title":"[docker] centos8 安装docker"},{"content":"1.什么是vagrant 具体定义大家可以参见官方网站: https://www.vagrantup.com/.\n那么大白话vagrant 到底是干嘛的, 它和docker有点像, 甚至部分操作用过docker 的人都会觉得似曾相识, 我们知道, docker 是来管理container的, 而vagrant是来管理虚拟机的, 比如你有个tomcat, 这个tomcat可以运行在docker容器上, 我们通过docker来管理这个容器, 而这个docker容器可以运行在VM上, 而vagrant可以是用来管理这个VM的, 当然, 这个只是一个宽泛的解释, 具体还是需要深入理解其本质.\n2.如何工作 vagrant 通过不同的provider 来对虚拟机软件的API进行调用, 例如 virtualbox, 那么vagrant 通过virtualbox provider 来操作VM. 也就省去了我们手动创建的麻烦 3. 安装  更多方式: https://www.vagrantup.com/downloads\n  Linux/Ubuntu  curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \u0026quot;deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026quot; sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install vagrant   Windows 这里注意，如果你使用virtualbox或者vmware,　并且运行你普通使用创建vm没问题，那么你只需要根据上面的链接安装vagrant, 如果你不要 因为vagrant需要hyper-v, 所以你要打开自己的windows hyper-v, 传送门 然后根据上面的下载地址，下载安装\n  MacOS\n  brew tap hashicorp/tap brew install vagrant 验证是否已经安装\n$ vagrant --version Vagrant 2.2.17 4. 下载基础镜像 这里我们以virtualbox 为例, 创建一个centos 7 的基础镜像，　那么我们需要现在vagrant上面下载: https://app.vagrantup.com/centos/boxes/7 点击这个镜像，可以看到我们的安装方式有vagrant file 和 new.　我们使用文件的方式, 本地新建一个文件夹用于vagrant, 然后创建这样一个文件，并执行安装.\n//创建Vagrantfile test@agv1vvp2:~/vagrant$ cat\u0026gt;Vagrantfile Vagrant.configure(\u0026quot;2\u0026quot;) do |config| config.vm.box = \u0026quot;centos/7\u0026quot; end //安装 test@1vvp2:~/vagrant$ vagrant up --provider=virtualbox --color Bringing machine 'default' up with 'virtualbox' provider... ==\u0026gt; default: Box 'centos/7' could not be found. Attempting to find and install... default: Box Provider: virtualbox default: Box Version: \u0026gt;= 0 ==\u0026gt; default: Loading metadata for box 'centos/7' default: URL: https://vagrantcloud.com/centos/7 ==\u0026gt; default: Adding box 'centos/7' (v2004.01) for provider: virtualbox default: Downloading: https://vagrantcloud.com/centos/boxes/7/versions/2004.01/providers/virtualbox.box Download redirected to host: cloud.centos.org default: Calculating and comparing box checksum... ==\u0026gt; default: Successfully added box 'centos/7' (v2004.01) for 'virtualbox'! ... ... //查看Vagrant 是否在运行 test@agv1vvp2:~/vagrant$ vagrant status Current machine states: default running (virtualbox) The VM is running. To stop this VM, you can run `vagrant halt` to shut it down forcefully, or you can run `vagrant suspend` to simply suspend the virtual machine. In either case, to restart it again, simply run `vagrant up`.  如果下载速度过慢，可以采用科学上网，或者添加新的镜像源, 源自己找就可以了，网上一堆:  $ vagrant box add centos7 \u0026lt;image link\u0026gt; 5. ssh到虚拟机  最快的方式  test@agv1vvp2:~/vagrant$ vagrant ssh Last login: Wed Jul 14 13:02:41 2021 from 10.0.2.2  传统ssh 的方式  有时我们需要传统的方式来ssh，那么我们需要先查看ssh 配置\ntest@agv1vvp2:~/vagrant$ vagrant ssh-config Host default HostName 127.0.0.1 User vagrant Port 2222 UserKnownHostsFile /dev/null StrictHostKeyChecking no PasswordAuthentication no IdentityFile /home/test/vagrant/.vagrant/machines/default/virtualbox/private_key IdentitiesOnly yes LogLevel FATAL 然后ssh\nssh vagrant@127.0.0.1 -p 2222 -o LogLevel=FATAL -o Compression=yes -o DSAAuthentication=yes -o IdentitiesOnly=yes -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i /home/test/vagrant/.vagrant/machines/default/virtualbox/private_key 或者用配置文件的方式\ntest@agv1vvp2:~/vagrant$ vagrant ssh-config \u0026gt; vagrant-ssh test@agv1vvp2:~/vagrant$ ssh -F vagrant-ssh default Last login: Wed Jul 14 13:01:37 2021 from 10.0.2.2 6. 配置自己的虚拟机 实际上, 这个Vagrantfile 就是一个Ruby 脚本, 它本身会调用Vagrant的API, 首先观察这个文件\nVagrant.configure(\u0026quot;2\u0026quot;) do |config| config.vm.box = \u0026quot;centos/7\u0026quot; end 可以看到, config.vm.box 这一行代码, 显而易见,这里就是指定了我们镜像的类型, 那么假设我们需要搭建多个节点, 我们同样可以扩展这个文件\nVagrant.configure(\u0026quot;2\u0026quot;) do |config| config.vm.box = \u0026quot;centos/7\u0026quot; //配置第一台vm, hostname为 server1 config.vm.define \u0026quot;node1\u0026quot; do |server1| server1.vm.hostname = \u0026quot;server1\u0026quot; end //配置第一台vm, hostname为 server2 config.vm.define \u0026quot;node2\u0026quot; do |server2| server2.vm.hostname = \u0026quot;server2\u0026quot; end //配置第一台vm, hostname为 server3 config.vm.define \u0026quot;node3\u0026quot; do |server3| server3.vm.hostname = \u0026quot;server3\u0026quot; end end 重新创建\ntest@agv1vvp2:~/vagrant$ vagrant up Bringing machine 'node1' up with 'virtualbox' provider... Bringing machine 'node2' up with 'virtualbox' provider... Bringing machine 'node3' up with 'virtualbox' provider... ... ... //查看其中一台vm test@agv1vvp2:~/vagrant$ vagrant ssh node1 [vagrant@server1 ~]$ exit 此时, 如果你打开你的virtualbox, 可以看到自己创建的三个文件\n 如果需要更多配置, 可以参考官方文档: https://www.vagrantup.com/docs/vagrantfile\n 7. 常用的命令  开机  vagrant up  关机  vagrant halt  关机并删除  vagrant destroy  ssh连接到虚拟机  vagrant ssh \u0026lt;host_name\u0026gt;  重新加载Vagrantfile  vagrant reload 赠瓶肥宅快乐水吧 ","permalink":"https://unprobug.com/post/vagrant/vagrant_for_beginners/","summary":"1.什么是vagrant 具体定义大家可以参见官方网站: https://www.vagrantup.com/.\n那么大白话vagrant 到底是干嘛的, 它和docker有点像, 甚至部分操作用过docker 的人都会觉得似曾相识, 我们知道, docker 是来管理container的, 而vagrant是来管理虚拟机的, 比如你有个tomcat, 这个tomcat可以运行在docker容器上, 我们通过docker来管理这个容器, 而这个docker容器可以运行在VM上, 而vagrant可以是用来管理这个VM的, 当然, 这个只是一个宽泛的解释, 具体还是需要深入理解其本质.\n2.如何工作 vagrant 通过不同的provider 来对虚拟机软件的API进行调用, 例如 virtualbox, 那么vagrant 通过virtualbox provider 来操作VM. 也就省去了我们手动创建的麻烦 3. 安装  更多方式: https://www.vagrantup.com/downloads\n  Linux/Ubuntu  curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \u0026quot;deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026quot; sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install vagrant   Windows 这里注意，如果你使用virtualbox或者vmware,　并且运行你普通使用创建vm没问题，那么你只需要根据上面的链接安装vagrant, 如果你不要 因为vagrant需要hyper-v, 所以你要打开自己的windows hyper-v, 传送门 然后根据上面的下载地址，下载安装","title":"[Vagrant] 入门"},{"content":"0. 简介 kustomize 形如customize, 也就是k8s+customize, 它是kube资源描述文件的一种抽象工具.\n 官方地址: https://kustomize.io/\n 1. 为什么会用 kustomize kustomize它作用类似Helm, 它的实现方式和helm不同, helm是基于模版, 而kustomize是基于overlay 引擎. 通俗讲就是说, 你写一个普通的k8s资源描述yaml文件, 那么所有东西都是写死的, 此时如果你有helm, 你可以把部分值写成变量模版, 达到灵活的目的. 而kustomize更像是代码式的声明将,基代码和定制化代码重叠, 达到灵活的目的, 有时我们的配置比较简单且固定, 此时, 我们更希望一种快捷易于操作的方式, 那么此时就可以考虑kustomize, 当然这不代表kustomize只能写简单的东西.\n我们来看一下需求是怎么来的:\n假设我们有一个pod.yaml:\n# pod.yaml contents apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: web image: web:v.1.0 一切看起来都很好, 突然有一天, 老板说, 我们客户给了一个新环境, 我们得再部署一套应用到名叫XINHUANJING的新环境, 此时你可能会说, 好办, 看我粘贴复制大法, 只需改点点配置.\n# pod.yaml contents apiVersion: v1 kind: Pod metadata: name: XINHUANJING-myapp-pod labels: app: myapp spec: containers: - name: web image: web:v.1.0 过了两天经理又来找你说, 我们的产品升级, 原来的k8s资源库也要更新, 把web app升级到2.0, 那么你可以继续粘贴复制, so easy!\n# pod.yaml contents apiVersion: v1 kind: Pod metadata: name: XINHUANJING-myapp-pod labels: app: myapp spec: containers: - name: web image: web:v.2.0 等等, 我还需要更新客户环境的描述文件夹, 粘贴复制 \u0026hellip;\n接下来的日子, 你发现随着业务量的增加, 客户的跟进, 乱七八糟deployment, statfulset \u0026hellip; 越来越多, 客户要的也不止改个名字那么简单, 此时粘贴复制好像就没那么好用了. 此时, 你便需要一个可以一通百通的方式来管理你的配置, 那么kustomize 和helm 就可以帮忙了.\n2. 入门案例 我们有两套环境, 我们需要分别在不同的开发环境(dev, prod)中, 配置不同的应用名\n2.1 基代码 //创建一个 基代码文件夹 test@master-0-test01:~\u0026gt; mkdir base //创建基代码 test@master-0-test01:~\u0026gt; cd base/ test@master-0-test01:~/base\u0026gt; ls kustomization.yaml pod.yaml //声明资源文件 test@master-0-test01:~/base\u0026gt; cat kustomization.yaml # kustomization.yaml contents resources: - pod.yaml //资源pod文件, 可以是各种k8s复杂的资源, deployment, resfulset, cm ... test@master-0-test01:~/base\u0026gt; cat pod.yaml # pod.yaml contents apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: nginx image: nginx:latest 此时, 你可以看出, 这个时候, 如果你直接kubectl apply 那么它会直接创建一个pod. 接下来我们来根据不同的环境改变这个pod\n2.2 写各个环境的特殊配置 假设我们有两个环境, 一个是开发环境 dev, 一个是 prod 环境, 我们希望 dev 环境的 pod 会标识出来 dev, 我们希望 prod 环境的 pod 会标识出来 prod\ntest@master-0-test01:~\u0026gt; mkdir overlays test@master-0-test01:~\u0026gt; cd overlays/ test@master-0-test01:~/overlays\u0026gt; ls dev prod test@master-0-test01:~/overlays\u0026gt; ls dev/ kustomization.yaml test@master-0-test01:~/overlays\u0026gt; ls prod/ kustomization.yaml //dev 环境的描述文件, 此时使用了kustomize的 namePrefix 标签 test@master-0-test01:~/overlays\u0026gt; cat dev/kustomization.yaml resources: - ../../base namePrefix: dev- //prod 环境的描述文件 test@master-0-test01:~/overlays\u0026gt; cat prod/kustomization.yaml resources: - ../../base namePrefix: prod- 可以看出, 描述文件使用了资源 ../../base, 这里类似我们写代码的import, 而且是个静态的import\n2.2 写一个总的入口文件 这时我们想看看生成的每个环境的描述文件是否正确, 此时我们需要在工作目录的根目录下创建一个总括的 kustomization.yaml\ntest@master-0-test01:~\u0026gt; cat kustomization.yaml resources: - ./overlays/dev - ./overlays/prod namePrefix: my-name- 可以看到, 我们的resources引入了 刚才我们创建的两个环境. 再看看此时的目录结构(我没装tree)\ntest@master-0-test01:~\u0026gt; pwd /home/test test@master-0-test01:~\u0026gt; ls base kustomization.yaml overlays test@master-0-test01:~\u0026gt; ls base kustomization.yaml pod.yaml test@master-0-test01:~\u0026gt; ls overlays/ dev prod 2.3 测试 执行 kustomize build \u0026lt;path-to-root-kustomization.yaml\u0026gt; 会生成预览文件(和helm 很类似), 可以看出我们的pod名当前前缀都已经变化\n// 如果你的集群没有kustomize, 可以下载: // curl -s \u0026quot;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026quot; | bash test@master-0-test01:~\u0026gt; kustomize build apiVersion: v1 kind: Pod metadata: labels: app: myapp name: my-name-dev-myapp-pod spec: containers: - image: nginx:latest name: nginx --- apiVersion: v1 kind: Pod metadata: labels: app: myapp name: my-name-prod-myapp-pod spec: containers: - image: nginx:latest name: nginx 2.4 创建pod test@master-0-test01:~\u0026gt; kubectl apply -k . pod/my-name-dev-myapp-pod created pod/my-name-prod-myapp-pod created 3. 总结 本例抛砖引玉, 实际生成也不可能只是个pod, 那么kustomize也提供了各种相关标签供大家使用: https://kubernetes.io/zh/docs/tasks/manage-kubernetes-objects/kustomization/\n赠瓶肥宅快乐水吧 ","permalink":"https://unprobug.com/post/kubernetes/how_to_use_kustomize/","summary":"0. 简介 kustomize 形如customize, 也就是k8s+customize, 它是kube资源描述文件的一种抽象工具.\n 官方地址: https://kustomize.io/\n 1. 为什么会用 kustomize kustomize它作用类似Helm, 它的实现方式和helm不同, helm是基于模版, 而kustomize是基于overlay 引擎. 通俗讲就是说, 你写一个普通的k8s资源描述yaml文件, 那么所有东西都是写死的, 此时如果你有helm, 你可以把部分值写成变量模版, 达到灵活的目的. 而kustomize更像是代码式的声明将,基代码和定制化代码重叠, 达到灵活的目的, 有时我们的配置比较简单且固定, 此时, 我们更希望一种快捷易于操作的方式, 那么此时就可以考虑kustomize, 当然这不代表kustomize只能写简单的东西.\n我们来看一下需求是怎么来的:\n假设我们有一个pod.yaml:\n# pod.yaml contents apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: web image: web:v.1.0 一切看起来都很好, 突然有一天, 老板说, 我们客户给了一个新环境, 我们得再部署一套应用到名叫XINHUANJING的新环境, 此时你可能会说, 好办, 看我粘贴复制大法, 只需改点点配置.\n# pod.yaml contents apiVersion: v1 kind: Pod metadata: name: XINHUANJING-myapp-pod labels: app: myapp spec: containers: - name: web image: web:v.","title":"[kubernetes] kustomize 入门例子"},{"content":"0. 搭建个人博客原料 github pages\n这里, 由于我们的个人博客是运行中github上, 使用过得是github提供的免费的github pages, 所以github是必要的\nHugo\nHugo是一个免费的go语言博客框架, 会帮助我们生成静态的html及相关脚本, 类似Hexo, 本教程使用的是Hugo框架\n1. 安装HUGO  Windows  打开Powershell, 右键管理员权限, 使用choco安装\n$ choco install hugo -confirm  Linux  打开Terminal, 使用snap安装\n$ sudo snap install hugo  更多安装方式, 请参考官方文档: https://gohugo.io/getting-started/installing/\n 2. 创建一个hugo github page 2.1 创建git page 的 repository Repository 就是一个普通的git 项目, git page 是通过解析项目来加载我们的静态网页文件, 小白的话, 就不必关心它是什么了\n 登录 https://github.com/, 点击 \u0026ldquo;Sign Up\u0026rdquo;, 根据提示完成github的注册 登录自己的github 创建一个repository, 注意这里需要填写和你用户名一致的repo名, 再跟上.github.io  选择public, 然后点击create repository 接下来你会看到一个Git操作指引, 这里先不要管, 我们先做后续的步骤   2.2 安装git 由于生成的git操作指引是需要安装git的, 那么我们需要先安装它, 安装信息: https://git-scm.com/downloads\n2.3 创建一个本地hugo 网页库 例如, 我在 ~/tmp/blog 下创建, 那么执行 hugo new site .\ntest@1vvp2:~/tmp/blog$ hugo new site . Congratulations! Your new Hugo site is created in /home/tmp/tmp/blog. Just a few more steps and you're ready to go: 1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/ or create your own with the \u0026quot;hugo new theme \u0026lt;THEMENAME\u0026gt;\u0026quot; command. 2. Perhaps you want to add some content. You can add single files with \u0026quot;hugo new \u0026lt;SECTIONNAME\u0026gt;/\u0026lt;FILENAME\u0026gt;.\u0026lt;FORMAT\u0026gt;\u0026quot;. 3. Start the built-in live server via \u0026quot;hugo server\u0026quot;. Visit https://gohugo.io/ for quickstart guide and full documentation. 此时如果你查看当前的文件夹, 会发现hugo 已经创建了所有需要的文件\ntest@1vvp2:~/tmp/blog$ ls archetypes config.toml content data layouts static themes 测试一下网页, 会提示本地网站运行在 http://localhost:1313/\ntest@1vvp2:~/tmp/blog$ hugo server 浏览器输入 http://localhost:1313/ 发现是空白页, 但是没有报错, 证明hugo运行正常, 空白的原因是我们没有加载主题\n2.3 使用Hugo主题 登录 https://themes.gohugo.io/tags/blog/ 选择一款自己喜欢的主题, 一般主题分Blog 和 个人主页, 如果是想突出个人, 就选择 个人主页, 如果是记录性质的, 选择 Blog 这里以cactus 为例\ntmp@e1vvp2:~/tmp/blog$ cd ~/tmp/blog //克隆主题到本地的/theme tmp@e1vvp2:~/tmp/blog$ git clone https://github.com/monkeyWzr/hugo-theme-cactus.git themes/cactus ... ... tmp@e1vvp2:~/tmp/blog/themes$ cd cactus/ tmp@e1vvp2:~/tmp/blog/themes/cactus$ ls assets exampleSite images layouts LICENSE README.md static theme.toml //将模版网页应用于本地 tmp@e1vvp2:~/tmp/blog/themes/cactus$ cp exampleSite/* ../../ 此时, 再次运行\nhugo server 登录 http://localhost:1313/, 你会发现现在的页面已经变了 2.4 创建新的页面 在hugo 的本地网页代码库中, content文件夹下创建文件夹 posts, 然后创建一个 test.md 的markdown文件输入如下内容\n--- title: \u0026quot;你好\u0026quot; tags: [\u0026quot;hello\u0026quot;] categories: [\u0026quot;hello\u0026quot;] --- 你好 此时再次查看 http://localhost:1313/, 你会发现你的页面已经出现了  通过 Ctrl+c 来停止正在运行的网页\n 2.5 将自己的页面和github page 连起来 这时我们再回头看看刚才github为我们创造的操作指南, 那么我们现在需要将本地的网页放在github上跑起来\n   生成静态页面 hugo -D    test@1vvp2:~/tmp/blog$ hugo -D Start building sites … hugo v0.85.0+extended linux/amd64 BuildDate=2021-07-05T14:34:48Z WARNING: calling IsSet with unsupported type \u0026quot;ptr\u0026quot; (*hugolib.SiteInfo) will always return false. | EN -------------------+----- Pages | 13 Paginator pages | 0 Non-page files | 0 Static files | 59 Processed images | 0 Aliases | 3 Sitemaps | 1 Cleaned | 0 Total in 56 ms //此时会生成一个静态网页文件夹public test@1vvp2:~/tmp/blog$ ls archetypes config.toml content data deploy.sh layouts netlify.toml public resources static themes   推送自己的网页到远端    // 进入public 文件夹, 执行初始化 git tmp@e1vvp2:~/tmp/blog/public$ git init Initialized empty Git repository in /home/tmp/tmp/blog/public/.git/ // 给定远端的博客地址 tmp@e1vvp2:~/tmp/blog/public$ git remote add origin git@github.com:JohnAndEthan/JohnAndEthan.github.io.git // 添加本地页面代码, 并提交 tmp@e1vvp2:~/tmp/blog/public$ git add . tmp@e1vvp2:~/tmp/blog/public$ git commit -m \u0026quot;我的博客\u0026quot; [master (root-commit) 054cd26] 我的博客 78 files changed, 11636 insertions(+) // 提交到远端库 tmp@e1vvp2:~/tmp/blog/public$ git branch -M main tmp@e1vvp2:~/tmp/blog/public$ git push -u origin main Counting objects: 110, done. Delta compression using up to 8 threads. Compressing objects: 100% (100/100), done. Writing objects: 100% (110/110), 3.11 MiB | 1.53 MiB/s, done. Total 110 (delta 23), reused 0 (delta 0) remote: Resolving deltas: 100% (23/23), done. To github.com:JohnAndEthan/JohnAndEthan.github.io.git * [new branch] main -\u0026gt; main Branch 'main' set up to track remote branch 'main' from 'origin'. 2.6 修改 baseURL 记得在 config.toml 中修改自己的baseURL 指向你的github page地址, 不然有可能页面不能加载主题\nbaseURL = \u0026quot;https://johnandethan.github.io/\u0026quot; 3. 测试 此时输入刚才我们 创建的repo地址, https://johnandethan.github.io/ 就可以看到我们的网页了\n4. 遇到问题怎么办  如果是失误操作, 直接删掉你本地的文件夹, 和github 的repository 重来就行了 如果是其他问题, 一般多数是由于主题的使用, 建议仔细阅读主题的相关文档  赠瓶肥宅快乐水吧 ","permalink":"https://unprobug.com/post/blog/hugo/how-to-create-blog/","summary":"","title":"[博客搭建 0] - 新建并挂载自己的博客到github page"},{"content":"0. 域名(Domain name)是什么 维基百科中定义: 网域名称（英语：Domain Name，简称：Domain），简称域名、网域，是由一串用点分隔的字符组成的互联网上某一台计算机或计算机组的名称，用于在数据传输时标识计算机的电子方位。域名可以说是一个IP地址的代称，目的是为了便于记忆后者。 大白话就是说, 用一个好记的名字来代表ip地址. 类似别人问你在哪, 你会说\u0026quot;我在北京路\u0026quot;, 而不会说我在北纬41°24'12.2 \u0026hellip;\n1. 为什么博客要Domain name 高端,大气,上档次\n2. 在哪儿买Domain name 这里就放两个常用的:\n 国内: https://wanwang.aliyun.com/domain/ 国外: https://www.godaddy.com/  3. 实例 (goDaddy) 由于我是用goDaddy, 我这里就直接用它来讲解\n3.1 搜索并购买域名 登录 https://hk.godaddy.com/ 实例并搜索你要的域名 根据自己的需要选定服务, 例如这里的全方位保护项, 戴保护肯定安全, 不戴后果自负, 反正我不爱戴 付账, 邮箱确认, 完毕\n3.2 绑定自己的git page 3.2.1 配置 A 记录 先说概念, A(Address)记录: 顾名思义就是地址, 也就是说, 假设你访问 google.com, 那么请求发出去后, 实际是访问google.com 的IP地址, 就是这个地址, 当然真正的访问过程要复杂得多, 这里简单理解即可, 配置位置 点击自己的域名, 既可以开始配置, 此时有可能要求你邮箱确认, 你确认一下 拖到最下面, 进入配置页面 点击编辑按钮, 编辑A记录, 此时配置地址 185.199.108.153, 这个地址就是 git page 的 IP 当前的ip地址包含:\n   GIT PAGE IPS     185.199.108.153   185.199.109.153   185.199.110.153   185.199.111.153    如果你觉得填一个不保险, 点击 \u0026ldquo;加入\u0026rdquo; 按钮, 填写方法和图里面的一样添加 185.199.109.153 \u0026hellip;\n3.2.2 将git page 和 domain name 连起来 登录自己的github page工程, 添加一个名为CNAME的文件 CNAME文件内容就是你自己的域名 3.3 测试 3.4 http 添加 ssl 证书 也就是 http -\u0026gt; https, 这一步需要在你自己的git page 工程下, 勾选 \u0026ldquo;Enforce HTTPS\u0026rdquo;, 这里需要一定的时间, 因为github 需要生成ssl证书给你 过一段时间后, 你便可以看到自己的博客协议已经是https, 不过, 这里有个证书信任问题, 我们可以后面在处理\n4. 结论 没结论\n赠瓶肥宅快乐水吧 ","permalink":"https://unprobug.com/post/blog/hugo/how-to-buy-a-domain-name/","summary":"","title":"[博客搭建 1] - 在goDaddy购买域名并绑定博客"},{"content":" 往往这些操作是需要特定权限的, 确保操作时你拥有操作权限\n 1. 暂停/取消暂停  这种状态下VM的状态会保存到RAM中, CUP则不会, 取消后会继续暂停前的状态  $ openstack server pause myInstance $ openstack server unpause myInstance 2. 挂起/取消挂起  这种状态类似物理机休眠, 状态会保存在文件中, CUP和RAM会被释放出来  $ openstack server pause myInstance $ openstack server resume myInstance 3. 开机/关机 $ openstack server start myInstance $ openstack server stop myInstance 4. 检查机器状态 4.1 简单查看, 即从所有实例中找目标机器 $ openstack server list | grep myInstance +--------------------------------------+-------------+---------+---------------------------------------------------------------------------------------------- | ID | Name | Status | Networks +--------------------------------------+-------------+---------+---------------------------------------------------------------------------------------------- | 27b48f01-aed0-47e6-b3ab-c22e1d7064f4 | master-01 | SHUTOFF | internal-net-01=10.0.10.75, fd00::17:1111::2f7 .... 4.2 查看详细信息 $ openstack server show master-01 +-------------------------------------+------------------------------------------------------------------+ | Field | Value | +-------------------------------------+------------------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | nova | | accessIPv4 | | | accessIPv6 | | | addresses | internal-net-01=10.0.10.75, fd00::17:1111::2f7 | | config_drive | True | | created | 2021-07-08T08:48:53Z | | flavor | large | | id | f4bbcae1-5e7a-4b72-929a-f3fdbd06fdb9 | | image | N/A (booted from volume) | | key_name | None | | name | master-01 | | progress | 0 | ... ... +-------------------------------------+------------------------------------------------------------------+  官方的server操作文档(pike)\n 赠瓶肥宅快乐水吧 ","permalink":"https://unprobug.com/post/openstack/openstack_server_status_operation_20210708/","summary":"往往这些操作是需要特定权限的, 确保操作时你拥有操作权限\n 1. 暂停/取消暂停  这种状态下VM的状态会保存到RAM中, CUP则不会, 取消后会继续暂停前的状态  $ openstack server pause myInstance $ openstack server unpause myInstance 2. 挂起/取消挂起  这种状态类似物理机休眠, 状态会保存在文件中, CUP和RAM会被释放出来  $ openstack server pause myInstance $ openstack server resume myInstance 3. 开机/关机 $ openstack server start myInstance $ openstack server stop myInstance 4. 检查机器状态 4.1 简单查看, 即从所有实例中找目标机器 $ openstack server list | grep myInstance +--------------------------------------+-------------+---------+---------------------------------------------------------------------------------------------- | ID | Name | Status | Networks +--------------------------------------+-------------+---------+---------------------------------------------------------------------------------------------- | 27b48f01-aed0-47e6-b3ab-c22e1d7064f4 | master-01 | SHUTOFF | internal-net-01=10.","title":"[openstack] 如何开关vm及类似操作命令"},{"content":"0. 为什么需要找到Blocking call 我们使用reactor编程时，其目的就是希望我们的程序符合异步非阻塞的模型，为了达到这个目的，我们希望我们程序中所有的方法都是非阻塞的方法(理想状态)，比如我们在处理JDBC链接时，会考虑使用Schedulers来包裹或是使用R2DBC，那么在响应式编程中，我们会遇到形形色色的阻塞方法，此时，我们就需要用合理的方式处理它们了.\n1. 解决方案 BlockHound\n2. Git 地址 https://github.com/reactor/BlockHound\n3. 大致原理 类似于Java代理，再入口函数调用前被JVM加载，一旦BlockHound启动，其将标记阻塞方法(例如: sleep()) .并改变其behaviour而抛出一个Error\n4. 引入BlockHound 在自己的工程中引入BlockHound\n4.1. maven \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.projectreactor.tools\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;blockhound-junit-platform\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.0.RC1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 4.2. Gradle repositories { mavenCentral() // maven { url 'https://repo.spring.io/milestone' } // maven { url 'https://repo.spring.io/snapshot' } } dependencies { testCompile 'io.projectreactor.tools:blockhound:$LATEST_RELEASE' // testCompile 'io.projectreactor.tools:blockhound:$LATEST_MILESTONE' // testCompile 'io.projectreactor.tools:blockhound:$LATEST_SNAPSHOT' } 5. 使用示例 public class DetectBlockingCall { @BeforeEach void setUp() { // 1. 初始化BlockHound BlockHound.install(); } // 2. 定义一个阻塞方法 void blockingCall() { Mono.delay(Duration.ofSeconds(1)) .doOnNext(it -\u0026gt; { try { Thread.sleep(10); } catch (InterruptedException e) { throw new RuntimeException(e); } }) .block(); } @Test void blockHoundSimpleTest() { //3. 调用阻塞方法 Throwable throwable = Assertions.assertThrows(Throwable.class, this::blockingCall); //4. 验证阻塞方法是否抛出异常 Assertions.assertTrue(throwable.getMessage().contains(\u0026quot;Blocking call!\u0026quot;)); } } 在这个示例中，第一步加载BlockHound实际是可以省略的，因为我们引入BlockHound到junit 实际是已经被预加载, 大家可以去除这一步再次执行测试代码尝试\n6. 构建项目时自动执行BlockHound 往往我们希望我们自身的项目可以自动执行BlockHound，从而每次运行测试代码便可以知道我们的代码问题在哪里，那么这里提供一种思路，即使用项目构建工具来执行BlockHound, 以Gradle为例.\n6.1. 编写定制化BlockHound模块 (当然你可以不定制化) 在开发中，往往我们不可避免的使用部分部分阻塞方法，那么此时我们需要测试时排除这些方法. 此时我们可以定义一些定制化类，例如:\n新建一个工程com.test.support， 新建一个模块叫做blockhound-integration, 然后新建一个Log的忽略类\npublic class LogBlockHoundIntegration implements BlockHoundIntegration { // 使用系统变量来达到开关的目的 private static final boolean ENABLED = Boolean.parseBoolean(System.getProperty(\u0026quot;LogBlockHoundIntegration.enabled\u0026quot;, Boolean.FALSE.toString())); @Override public void applyTo(BlockHound.Builder builder) { if (!ENABLED) { return; } // 加入要忽略的阻塞方法 builder.allowBlockingCallsInside( \u0026quot;ch.qos.logback.classic.Logger\u0026quot;, \u0026quot;buildLoggingEventAndAppend\u0026quot;); } } 6.2. 定义测试监听类 实现TestExecutionListener, 静态加载BlockHound，使得所有测试方法都需要加载BlockHound\npublic class BlockHoundTestExecutionListener implements TestExecutionListener { static { BlockHound.install(builder -\u0026gt; { builder.blockingMethodCallback(method -\u0026gt; { Error error = new BlockingOperationError(method); error.printStackTrace(System.err); throw error; }); }); } } 6.3. 在自己模块的gradle文件中定义方法，引入我们的定义及默认的junit平台 ext { // add helper to activate Reactor BlockHound, https://github.com/reactor/BlockHound useReactorBlockHound = { -\u0026gt; project.dependencies { testRuntimeOnly 'com.test.support:blockhound-integration', 'org.junit.platform:junit-platform-launcher' } } } 6.4. 定义执行操作入口 build.gradle 中插入\nsubprojects { subproject -\u0026gt; subproject.useReactorBlockHound() } // 打开我们自己定义的生效类 tasks.withType(Test) { // ignore the blocking nature of Log systemProperty 'LogBlockHoundIntegration.enabled', 'true' } 至此，我们基本可以满足gradle项目开发中所需要的自动化测试了。如果你在使用maven，可以构建自己的maven插件，来实现自动化流程，具体逻辑与gradle是类似的\n6. 结论 响应式编程是基于我们想充分利用异步非阻塞而产生的一种设计，但如今我理解技术正处于一个转型期，往往我们会遇到阻塞+非阻塞的囧境，为了解决这个问题，今天引入BlockHound工具来探测我们程序中潜在的阻塞API，使我们更快的发现问题并做出调整.\n7. ref https://medium.com/@domenicosibilio/blockhound-detect-blocking-calls-in-reactive-code-before-its-too-late-6472f8ad50c1\n赠瓶肥宅快乐水吧 ","permalink":"https://unprobug.com/post/java/spring/%E6%89%BE%E5%87%BAwebflux%E4%B8%AD%E7%9A%84%E9%98%BB%E5%A1%9E%E6%96%B9%E6%B3%95/","summary":"","title":"[Spring][Webflux]如何找出webflux中的阻塞方法"},{"content":"基本信息  作者: Jiang WU 权利人: Jiang WU 联系方式: johnwufr@gmail.com  版权声明  本版权声明承袭著作权法, 在此基础上额外声明以下规定. 本网站文章, 帖子等仅代表作者本人的观点, 本站不保证文章等内容的有效性. 属在本网站发表的文章（包括转帖）, 版权归原作者所有. 本网站会不定期的对本站的著作进行审查, 对于收录的文章, 会首先征求作者同意 本网站部分内容转载于合作站点或其他站点,但都会注明作/译者和原出处. 如有不妥之处,敬请指出 在征得本网站,以及作, 译者同意的情况下,本网站的作品允许非盈利性引用,于本站原创作品并请注明出处：\u0026ldquo;作者：Jiang WU 转载自 unprobug.com\u0026quot;字样, 于非本站原创作品请联系创作者版权问题, 以尊重作者的劳动成. 版权归原作/译者所. 未经允许,严禁转载. 对非法转载者,其行为包括未经允许的引用, 转载, 复制的片段, 改编, 演绎, 二次转载, 本站和作/译者保留采用法律手段追究的权利.  授权许可  非商业用途, 默认许可, 但需遵循以上版权声明 商业用途, 需要征得本网站许可, 并获得本网站签发的许可证书, 方可获得授权  商业洽谈  请通过邮箱联系本站管理员  免责声明  本站不负责在本站留言中的恶意诋毁, 污蔑, 及网络攻击等违法行为造成的后果  ","permalink":"https://unprobug.com/copyright/","summary":"基本信息  作者: Jiang WU 权利人: Jiang WU 联系方式: johnwufr@gmail.com  版权声明  本版权声明承袭著作权法, 在此基础上额外声明以下规定. 本网站文章, 帖子等仅代表作者本人的观点, 本站不保证文章等内容的有效性. 属在本网站发表的文章（包括转帖）, 版权归原作者所有. 本网站会不定期的对本站的著作进行审查, 对于收录的文章, 会首先征求作者同意 本网站部分内容转载于合作站点或其他站点,但都会注明作/译者和原出处. 如有不妥之处,敬请指出 在征得本网站,以及作, 译者同意的情况下,本网站的作品允许非盈利性引用,于本站原创作品并请注明出处：\u0026ldquo;作者：Jiang WU 转载自 unprobug.com\u0026quot;字样, 于非本站原创作品请联系创作者版权问题, 以尊重作者的劳动成. 版权归原作/译者所. 未经允许,严禁转载. 对非法转载者,其行为包括未经允许的引用, 转载, 复制的片段, 改编, 演绎, 二次转载, 本站和作/译者保留采用法律手段追究的权利.  授权许可  非商业用途, 默认许可, 但需遵循以上版权声明 商业用途, 需要征得本网站许可, 并获得本网站签发的许可证书, 方可获得授权  商业洽谈  请通过邮箱联系本站管理员  免责声明  本站不负责在本站留言中的恶意诋毁, 污蔑, 及网络攻击等违法行为造成的后果  ","title":"Copyright"}]