{"posts":[{"title":"[ansible]add_host模块使用","text":"需求有时在使用ansible时, 有一些需求需要动态的加入一些host, 比如临时创建两台虚拟机, 在创建之前你也不知道他们的ip, 而在创建之后, 你需要用ansible进行一系列操作.这时把host写进内存就是一个比较好的方法. add_host 模块这个模块就是ansible预置的模块, 提供动态添加host, 更直接的讲就是可以直接修改inventory文件, 当然这个inventory 文件并不是真正你使用的inventory, 而是内存中的inventory 例子测试环境原本的inventory 文件 123[new]192.168.50.146192.168.50.109 这时, 写一个简单的play 12345678910---- name: hello world hosts: new gather_facts: false tasks: - name: test current hosts debug: msg: &quot;{{item}} {{hostvars[item].inventory_hostname}}&quot; loop: &quot;{{ play_hosts }}&quot; 执行 1ansible-playbook test_add_host.yml -v 结果 12345678910TASK [test current hosts] **************************************************************************************************************************************************************skipping: [192.168.50.146] =&gt; (item=192.168.50.146) =&gt; {&quot;ansible_loop_var&quot;: &quot;item&quot;, &quot;item&quot;: &quot;192.168.50.146&quot;}ok: [192.168.50.146] =&gt; (item=192.168.50.109) =&gt; { &quot;msg&quot;: &quot;192.168.50.109 192.168.50.109&quot;}ok: [192.168.50.109] =&gt; (item=192.168.50.146) =&gt; { &quot;msg&quot;: &quot;192.168.50.146 192.168.50.146&quot;}skipping: [192.168.50.109] =&gt; (item=192.168.50.109) =&gt; {&quot;ansible_loop_var&quot;: &quot;item&quot;, &quot;item&quot;: &quot;192.168.50.109&quot;} 可以看出这里我们有了两个最基本的host, 也就是 group ‘new’ 中定义的两个节点 使用add_host123456789101112131415161718192021222324252627282930313233343536---- name: hello world hosts: new gather_facts: false tasks: # 添加一个 ip 为192.168.50.109 的host 到 just_created_first, 并给定给一个变量 foo - name: Add host IP to group 'just_created_first' with variable foo=42 add_host: name: 192.168.50.109 groups: just_created_first foo: 42 # 添加一个 hostname 为node_in_memory 的host 到 just_created_second, 并指定链接方式为ssh - name: Add host name to group 'just_created_second' with variable add_host: name: node_in_memory ansible_host: &quot;192.168.50.146&quot; ansible_connection: ssh groups: just_created_second # 打印此时的 play_hosts - name: test current hosts debug: msg: &quot;{{item}} {{hostvars[item].inventory_hostname}}&quot; when: item != inventory_hostname loop: &quot;{{ play_hosts }}&quot; # 打印此时的 ansible_play_hosts - name: test current ansible hosts in memory debug: msg: &quot;ansible_play_hosts : {{item}}, in inventory_dir: {{ inventory_dir }}&quot; loop: &quot;{{ ansible_play_hosts }}&quot; # 查看当前所有的host变量 - name: use hostname node_in_memory delegate_to: &quot;{{ groups['just_created_second'][0] }}&quot; run_once: true debug: msg: &quot;{{ hostvars }}&quot; 这里很多个步骤, 我们只看最后一个task的输出结果, 前面的task也只是为了分别查看部分变量. 最后一个打印了所有我们当前的host变量值, 也就是我们可以使用的host地址, 参数, 等等, 我们可以使用这些参数来进行操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155TASK [test host vars node_in_memory] ***************************************************************************************************************************************************ok: [192.168.50.146 -&gt; 192.168.50.146] =&gt; { &quot;msg&quot;: { &quot;192.168.50.109&quot;: { &quot;ansible_check_mode&quot;: false, &quot;ansible_diff_mode&quot;: false, &quot;ansible_facts&quot;: {}, &quot;ansible_forks&quot;: 5, &quot;ansible_inventory_sources&quot;: [ &quot;/home/osboxes/ansible/helloworld/hosts&quot; ], &quot;ansible_playbook_python&quot;: &quot;/usr/bin/python2&quot;, &quot;ansible_run_tags&quot;: [ &quot;all&quot; ], &quot;ansible_skip_tags&quot;: [], &quot;ansible_sudo_pass&quot;: &quot;osboxes.org&quot;, &quot;ansible_verbosity&quot;: 1, &quot;ansible_version&quot;: { &quot;full&quot;: &quot;2.9.18&quot;, &quot;major&quot;: 2, &quot;minor&quot;: 9, &quot;revision&quot;: 18, &quot;string&quot;: &quot;2.9.18&quot; }, &quot;foo&quot;: 42, &quot;group_names&quot;: [ &quot;just_created_first&quot;, &quot;new&quot; ], &quot;groups&quot;: { &quot;all&quot;: [ &quot;node_in_memory&quot;, &quot;192.168.50.146&quot;, &quot;192.168.50.109&quot; ], &quot;just_created_first&quot;: [ &quot;192.168.50.109&quot; ], &quot;just_created_second&quot;: [ &quot;node_in_memory&quot; ], &quot;new&quot;: [ &quot;192.168.50.146&quot;, &quot;192.168.50.109&quot; ], &quot;ungrouped&quot;: [] }, &quot;inventory_dir&quot;: &quot;/home/osboxes/ansible/helloworld&quot;, &quot;inventory_file&quot;: &quot;/home/osboxes/ansible/helloworld/hosts&quot;, &quot;inventory_hostname&quot;: &quot;192.168.50.109&quot;, &quot;inventory_hostname_short&quot;: &quot;192&quot;, &quot;playbook_dir&quot;: &quot;/home/osboxes/ansible/helloworld&quot; }, &quot;192.168.50.146&quot;: { &quot;ansible_check_mode&quot;: false, &quot;ansible_diff_mode&quot;: false, &quot;ansible_facts&quot;: {}, &quot;ansible_forks&quot;: 5, &quot;ansible_inventory_sources&quot;: [ &quot;/home/osboxes/ansible/helloworld/hosts&quot; ], &quot;ansible_playbook_python&quot;: &quot;/usr/bin/python2&quot;, &quot;ansible_run_tags&quot;: [ &quot;all&quot; ], &quot;ansible_skip_tags&quot;: [], &quot;ansible_sudo_pass&quot;: &quot;osboxes.org&quot;, &quot;ansible_verbosity&quot;: 1, &quot;ansible_version&quot;: { &quot;full&quot;: &quot;2.9.18&quot;, &quot;major&quot;: 2, &quot;minor&quot;: 9, &quot;revision&quot;: 18, &quot;string&quot;: &quot;2.9.18&quot; }, &quot;group_names&quot;: [ &quot;new&quot; ], &quot;groups&quot;: { &quot;all&quot;: [ &quot;node_in_memory&quot;, &quot;192.168.50.146&quot;, &quot;192.168.50.109&quot; ], &quot;just_created_first&quot;: [ &quot;192.168.50.109&quot; ], &quot;just_created_second&quot;: [ &quot;node_in_memory&quot; ], &quot;new&quot;: [ &quot;192.168.50.146&quot;, &quot;192.168.50.109&quot; ], &quot;ungrouped&quot;: [] }, &quot;inventory_dir&quot;: &quot;/home/osboxes/ansible/helloworld&quot;, &quot;inventory_file&quot;: &quot;/home/osboxes/ansible/helloworld/hosts&quot;, &quot;inventory_hostname&quot;: &quot;192.168.50.146&quot;, &quot;inventory_hostname_short&quot;: &quot;192&quot;, &quot;playbook_dir&quot;: &quot;/home/osboxes/ansible/helloworld&quot; }, &quot;node_in_memory&quot;: { &quot;ansible_check_mode&quot;: false, &quot;ansible_connection&quot;: &quot;ssh&quot;, &quot;ansible_diff_mode&quot;: false, &quot;ansible_facts&quot;: {}, &quot;ansible_forks&quot;: 5, &quot;ansible_host&quot;: &quot;192.168.50.146&quot;, &quot;ansible_inventory_sources&quot;: [ &quot;/home/osboxes/ansible/helloworld/hosts&quot; ], &quot;ansible_playbook_python&quot;: &quot;/usr/bin/python2&quot;, &quot;ansible_run_tags&quot;: [ &quot;all&quot; ], &quot;ansible_skip_tags&quot;: [], &quot;ansible_verbosity&quot;: 1, &quot;ansible_version&quot;: { &quot;full&quot;: &quot;2.9.18&quot;, &quot;major&quot;: 2, &quot;minor&quot;: 9, &quot;revision&quot;: 18, &quot;string&quot;: &quot;2.9.18&quot; }, &quot;group_names&quot;: [ &quot;just_created_second&quot; ], &quot;groups&quot;: { &quot;all&quot;: [ &quot;node_in_memory&quot;, &quot;192.168.50.146&quot;, &quot;192.168.50.109&quot; ], &quot;just_created_first&quot;: [ &quot;192.168.50.109&quot; ], &quot;just_created_second&quot;: [ &quot;node_in_memory&quot; ], &quot;new&quot;: [ &quot;192.168.50.146&quot;, &quot;192.168.50.109&quot; ], &quot;ungrouped&quot;: [] }, &quot;inventory_dir&quot;: null, &quot;inventory_file&quot;: null, &quot;inventory_hostname&quot;: &quot;node_in_memory&quot;, &quot;inventory_hostname_short&quot;: &quot;node_in_memory&quot;, &quot;playbook_dir&quot;: &quot;/home/osboxes/ansible/helloworld&quot; } }} 可以看出, 此时,我们每个节点的host变量集合, 对于每台机器我们都有如下组别, 这里有一个我们特意使用node_in_memory的hostname, 没有使用节点ip,依然可以链接,原因就是我们在给定name时,也给定了ansible_host, 也就是ansible中 host文件的本身支持的变量. 123456789101112131415161718&quot;groups&quot;: { &quot;all&quot;: [ &quot;node_in_memory&quot;, &quot;192.168.50.146&quot;, &quot;192.168.50.109&quot; ], &quot;just_created_first&quot;: [ &quot;192.168.50.109&quot; ], &quot;just_created_second&quot;: [ &quot;node_in_memory&quot; ], &quot;new&quot;: [ &quot;192.168.50.146&quot;, &quot;192.168.50.109&quot; ], &quot;ungrouped&quot;: []} 除了ansible_host, 我们也可以指定, ansible链接时的参数, 以ssh为例, 此时我们可以指定各种参数, 例如 12345678910111213- name: Add hosts add_host: groups: &quot;{{ myhost.groups }}&quot; name: &quot;{{ myhost.name }}&quot; ansible_host: &quot;{{ myhost.address }}&quot; ansible_connection: ssh ansible_ssh_user: &quot;{{ myhost.user }}&quot; ansible_ssh_extra_args: &quot;-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null&quot; ansible_ssh_private_key_file: &quot;{{ myhost.ssh_priv_key_path }}&quot; ansible_ssh_common_args: &gt;- -o ProxyCommand='ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -W %h:%p -q -i {{ myhost.proxy.ssh_priv_key_path }} {{ myhost.proxy.user }}@{{ myhost.proxy.host }}' 结论add_host解决了临时添加host的问题, 但由于其是写入内存中, 所以在使用时需要开发人员知晓当前的host信息, 所以开发中需要注意, 特别是当使用临时的组名, 很容易不知道当前这个组包含的节点, 所以注意灵活使用”“来查看当前的inventory信息 Reference https://docs.ansible.com/ansible/latest/collections/ansible/builtin/add_host_module.html","link":"/2021/10/07/ansible/how-to-add-host/"},{"title":"[ansible]key value list 转 dictionary","text":"需求这两天我需要kubernetes 上面没有ready 的pod列出来, 然后对这些pod进行操作, 又要用到ansible, 思来想去不如来个map, 那么 python 或者ansible里面叫dict 输入这里有一个 kubectl get pods 得出的结果, 使用ansible的 shell, 我们需要处理其中的stdout, 或者stdout_lines 123456789101112131415161718192021222324252627282930313233changed: [localhost -&gt; fd00:eccd:0:a0a::5] =&gt; { &quot;ansible_facts&quot;: { &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot; }, &quot;changed&quot;: true, &quot;cmd&quot;: &quot;/usr/local/bin/kubectl get pods -n kube-system --field-selector spec.nodeName=poolalpha-worker-0-efggjjp-ansibd-01 -o custom-columns=NAME:.metadata.name,\\&quot;IS_READY\\&quot;:.status.containerStatuses[].ready --no-headers&quot;, &quot;delta&quot;: &quot;0:00:00.475861&quot;, &quot;end&quot;: &quot;2021-07-29 19:37:41.515968&quot;, &quot;invocation&quot;: { &quot;module_args&quot;: { &quot;_raw_params&quot;: &quot;/usr/local/bin/kubectl get pods -n kube-system --field-selector spec.nodeName=poolalpha-worker-0-efggjjp-ansibd-01 -o custom-columns=NAME:.metadata.name,\\&quot;IS_READY\\&quot;:.status.containerStatuses[].ready --no-headers&quot;, &quot;_uses_shell&quot;: true, &quot;argv&quot;: null, &quot;chdir&quot;: null, &quot;creates&quot;: null, &quot;executable&quot;: null, &quot;removes&quot;: null, &quot;stdin&quot;: null, &quot;stdin_add_newline&quot;: true, &quot;strip_empty_ends&quot;: true, &quot;warn&quot;: true } }, &quot;rc&quot;: 0, &quot;start&quot;: &quot;2021-07-29 19:37:41.040107&quot;, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], ... &quot;stdout_lines&quot;: [ &quot;nginx-deployment-66b6c48dd5-4crpt true&quot;, &quot;nginx-deployment-66b6c48dd5-hqlxm false&quot; ]} 需要的数据结构我需要得到一个dict, pod_status, 然后key 是pod名, value是pod(容器)状态, 也就是这样滴-&gt; 1234&quot;pod_status&quot;: { &quot;nginx-deployment-66b6c48dd5-4crpt&quot;: &quot;true&quot;, &quot;nginx-deployment-66b6c48dd5-hqlxm&quot;: &quot;false&quot; } 具体实现1234567891011121314151617---- name: test key value list to dict hosts: localhost gather_facts: false tasks: - name: set values set_fact: status_list: - &quot;nginx-deployment-66b6c48dd5-4crpt true&quot; - &quot;nginx-deployment-66b6c48dd5-hqlxm false&quot; - name: Set pod-status dict vars: pod: &quot;{{ item.split()[0]|trim }}&quot; status: &quot;{{ item.split()[1]|trim }}&quot; set_fact: pod_status: &quot;{{ pod_status | default({}) | combine({pod : status})}}&quot; loop: &quot;{{ status_list }}&quot; 这里使用了combine 函数+var 配合的方法, loop循环status_list, 所以会迭代两个item “nginx-deployment-66b6c48dd5-4crpt true” “nginx-deployment-66b6c48dd5-4crpt true” 这时再使用.split()来切割字符串, 那么其就会变成一个长度为 2 的数组, 也就是 item.split()[0] 和 item.split()[1], 里面分别装pod名和状态. 然后他们会被赋值到vars 然后通过combine函数生成数据结构为{pod : status} 的map, 也就是dictionary, 因为combine是merge的规则, 所以迭代后会叠加下去, 也就生成了 1234&quot;pod_status&quot;: { &quot;nginx-deployment-66b6c48dd5-4crpt&quot;: &quot;true&quot;, &quot;nginx-deployment-66b6c48dd5-hqlxm&quot;: &quot;false&quot; }","link":"/2021/07/30/ansible/key_value_list_to_dict/"},{"title":"[centos8] 使用DNF安装git","text":"前言这里使用dnf, dnf是下一代的包管理工具 安装git安装dnf 1$ sudo dnf update -y 安装git 1$ sudo dnf install git -y 验证 12$ git --versiongit version 2.27.0 简单配置git1$ vim ~/.gitconfig 配置用户信息 123[user] name = developer email = developer@domain.com 验证 1234$ git config --listuser.name=developeruser.email=developer@domain.com","link":"/2021/08/02/centos/install_git_centos8/"},{"title":"[docker] centos8 安装docker","text":"1. 前置条件准备一台centos 8 的机器, 我这里用的是vagrant安装的centos8镜像 2. 安装过程2.1 测试是否链接外网123456789$ ping google.comPING google.com (216.58.211.14) 56(84) bytes of data.64 bytes from muc03s13-in-f14.1e100.net (216.58.211.14): icmp_seq=1 ttl=63 time=11.5 ms64 bytes from muc03s13-in-f14.1e100.net (216.58.211.14): icmp_seq=2 ttl=63 time=12.1 ms^C--- google.com ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 2msrtt min/avg/max/mdev = 11.476/11.781/12.086/0.305 ms 2.2 删除旧的docker12345678$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 2.3 安装需要的依赖包1$ sudo yum install -y yum-utils 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556//OUTPUTCentOS Linux 8 - AppStream 7.4 MB/s | 8.1 MB 00:01 CentOS Linux 8 - BaseOS 4.7 MB/s | 3.6 MB 00:00 CentOS Linux 8 - Extras 38 kB/s | 9.8 kB 00:00 Package yum-utils-4.0.17-5.el8.noarch is already installed.Dependencies resolved.========================================================================================================================================================================================= Package Architecture Version Repository Size=========================================================================================================================================================================================Upgrading: dnf-plugins-core noarch 4.0.18-4.el8 baseos 69 k python3-dnf-plugins-core noarch 4.0.18-4.el8 baseos 234 k yum-utils noarch 4.0.18-4.el8 baseos 71 kTransaction Summary=========================================================================================================================================================================================Upgrade 3 PackagesTotal download size: 375 kDownloading Packages:(1/3): yum-utils-4.0.18-4.el8.noarch.rpm 1.3 MB/s | 71 kB 00:00 (2/3): dnf-plugins-core-4.0.18-4.el8.noarch.rpm 1.2 MB/s | 69 kB 00:00 (3/3): python3-dnf-plugins-core-4.0.18-4.el8.noarch.rpm 3.5 MB/s | 234 kB 00:00 -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Total 858 kB/s | 375 kB 00:00 warning: /var/cache/dnf/baseos-31c79d9833c65cf7/packages/dnf-plugins-core-4.0.18-4.el8.noarch.rpm: Header V3 RSA/SHA256 Signature, key ID 8483c65d: NOKEYCentOS Linux 8 - BaseOS 1.6 MB/s | 1.6 kB 00:00 Importing GPG key 0x8483C65D: Userid : &quot;CentOS (CentOS Official Signing Key) &lt;security@centos.org&gt;&quot; Fingerprint: 99DB 70FA E1D7 CE22 7FB6 4882 05B5 55B3 8483 C65D From : /etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficialKey imported successfullyRunning transaction checkTransaction check succeeded.Running transaction testTransaction test succeeded.Running transaction Preparing : 1/1 Upgrading : python3-dnf-plugins-core-4.0.18-4.el8.noarch 1/6 Upgrading : dnf-plugins-core-4.0.18-4.el8.noarch 2/6 Upgrading : yum-utils-4.0.18-4.el8.noarch 3/6 Cleanup : yum-utils-4.0.17-5.el8.noarch 4/6 Cleanup : dnf-plugins-core-4.0.17-5.el8.noarch 5/6 Cleanup : python3-dnf-plugins-core-4.0.17-5.el8.noarch 6/6 Running scriptlet: python3-dnf-plugins-core-4.0.17-5.el8.noarch 6/6 Verifying : dnf-plugins-core-4.0.18-4.el8.noarch 1/6 Verifying : dnf-plugins-core-4.0.17-5.el8.noarch 2/6 Verifying : python3-dnf-plugins-core-4.0.18-4.el8.noarch 3/6 Verifying : python3-dnf-plugins-core-4.0.17-5.el8.noarch 4/6 Verifying : yum-utils-4.0.18-4.el8.noarch 5/6 Verifying : yum-utils-4.0.17-5.el8.noarch 6/6 Upgraded: dnf-plugins-core-4.0.18-4.el8.noarch python3-dnf-plugins-core-4.0.18-4.el8.noarch yum-utils-4.0.18-4.el8.noarch Complete! 2.4 添加docker repo123$ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 2.5 安装docker1$ sudo yum install docker-ce docker-ce-cli containerd.io 3. 验证重启docker 1$ sudo systemctl start docker 验证 123456789101112131415161718192021222324252627[vagrant@localhost ~]$ sudo docker run hello-worldUnable to find image 'hello-world:latest' locallylatest: Pulling from library/hello-worldb8dfde127a29: Pull complete Digest: sha256:df5f5184104426b65967e016ff2ac0bfcd44ad7899ca3bbcf8e44e4461491a9eStatus: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/ 4. Ref:https://docs.docker.com/engine/install/centos/","link":"/2021/07/19/docker/how_to_install_docker_in_centos8/"},{"title":"链接pod时 出现Error from server error dialing backend remote error - tls - internal error","text":"问题出现发现这个问题是因为我需要查看部分CrashLoopBackOff 的pod日志, 当执行 1$ kubectl -n kube-system exec -it openstack-cloud-controller-manager-vrdld -- /bin/bash 时报错, 错误如下: 1Error from server: error dialing backend: remote error: tls: internal error 问题分析看到tls, 很明显就是来自证书问题, 猜测是证书制作有问题, 那么kubernetes中的证书是由kubernetes的CA签发, 这时可以通过curl 来猜测一下: 1234567891011$ curl -v https://[fd00:1111:94:1111::12]:10250/containerLogs/kube-system/openstack-cloud-controller-manager-xqqm5/openstack-cloud-controller-manager* Trying fd00:1111:94:1111::12:10250...* TCP_NODELAY set* Connected to fd00:eccd:94:1111::12 (fd00:1111:94:1111::12) port 10250 (#0)* ALPN, offering h2* ALPN, offering http/1.1* TLSv1.3 (OUT), TLS handshake, Client hello (1):* TLSv1.3 (IN), TLS alert, internal error (592):* error:14094438:SSL routines:ssl3_read_bytes:tlsv1 alert internal error* Closing connection 0curl: (35) error:14094438:SSL routines:ssl3_read_bytes:tlsv1 alert internal error 可以看到这里错误 error:14094438:SSL routines:ssl3_read_bytes:tlsv1 alert internal error 此时查询所有证书 1234567$ kubectl get csrNAME AGE SIGNERNAME REQUESTOR CONDITIONcsr-27xqp 55m kubernetes.io/kubelet-serving system:node:master-2-efggjjp-ansibd-01 Pendingcsr-28w9n 19h kubernetes.io/kubelet-serving system:node:master-2-efggjjp-ansibd-01 Pendingcsr-2gdwc 20h kubernetes.io/kubelet-serving system:node:master-2-efggjjp-ansibd-01 Pendingcsr-2hq2x 6h5m kubernetes.io/kubelet-serving system:node:master-0-efggjjp-ansibd-01 Pending... ... 发现所有证书都在pending, 此时我们需要批准证书 解决问题kubernetes 提供给管理员直接approve 的api: 1$ kubectl certificate approve &lt;certificat name&gt; 写个脚本批准所有 12345#!/bin/bashfor i in $(kubectl get csr |grep Pending| awk '{print $1}'); do kubectl certificate approve $idone 再次通过 kubectl logs 或者 kubectl exec 尝试接入pod 1234$ kubectl -n kube-system logs openstack-cloud-controller-manager-xqqm5I1004 10:27:34.324881 1 flags.go:59] FLAG: --add-dir-header=&quot;false&quot;I1004 10:27:34.324929 1 flags.go:59] FLAG: --add_dir_header=&quot;false&quot;... ...","link":"/2021/10/04/kubernetes/how-to-approve-certificate/"},{"title":"[kubernetes]如何查看node的kubelet配置","text":"0. 前言有时因为一些原因, 我们需要修改kubelet 的配置, 新版本的kubernetes就是修改 config.yml, 那么这里有一个问题, 你怎么知道你的kubelet 已经生效了? 1. 先说结论查询方法为先开启kube proxy然后使用 kubernetes 提供的查询接口 1api/vi/nodes/&lt;node_name&gt;/proxy/cofigz 2. 举个栗子例如我们有这样一个集群: 1234567test@node-01:~&gt; kubectl get nodes -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEmaster-0-01 Ready control-plane,master 16h v1.21.1 10.0.10.9 &lt;none&gt; SUSE Linux Enterprise Server 15 SP2 5.3.18-24.67-default containerd://1.4.4master-1-01 Ready control-plane,master 16h v1.21.1 10.0.10.10 &lt;none&gt; SUSE Linux Enterprise Server 15 SP2 5.3.18-24.67-default containerd://1.4.4master-2-01 Ready control-plane,master 16h v1.21.1 10.0.10.24 &lt;none&gt; SUSE Linux Enterprise Server 15 SP2 5.3.18-24.67-default containerd://1.4.4poolalpha-worker-0-01 Ready worker 15h v1.21.1 10.0.10.4 &lt;none&gt; SUSE Linux Enterprise Server 15 SP2 5.3.18-24.67-default containerd://1.4.4poolalpha-worker-1-01 Ready worker 15h v1.21.1 10.0.10.13 &lt;none&gt; SUSE Linux Enterprise Server 15 SP2 5.3.18-24.67-default containerd://1.4.4 2.1 修改kubelet配置接下来, 我们希望修改poolalpha-worker-0-01 的kubelet某个参数, 那么我们先登录这个节点, 然后查看当前节点使用的config.yml 1234567891011121314test@poolalpha-worker-0-01:~&gt; systemctl status kubelet● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/local/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2021-07-26 15:44:19 UTC; 15h ago Docs: http://kubernetes.io/docs/ Process: 8868 ExecStopPost=/bin/umount --verbose /opt/cni (code=exited, status=0/SUCCESS) Process: 8874 ExecStartPre=/bin/mount --verbose --bind /usr/local/lib/cni /opt/cni (code=exited, status=0/SUCCESS) Process: 8869 ExecStartPre=/bin/mkdir --verbose --parents /opt/cni (code=exited, status=0/SUCCESS) Main PID: 8878 (kubelet) Tasks: 16 CGroup: /system.slice/kubelet.service └─8878 /usr/local/bin/kubelet ##### 这里可以看到kubelet使用的参数文件 --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network-plugin=cn&gt; 我们观察到, 此时这个节点使用的配置文件为 –config=/var/lib/kubelet/config.yaml, 打开看一下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647test@poolalpha-worker-0-01:~&gt; cat /var/lib/kubelet/config.yaml apiVersion: kubelet.config.k8s.io/v1beta1authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.crtauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 0s cacheUnauthorizedTTL: 0scgroupDriver: systemdclusterDNS:- 169.254.20.10clusterDomain: cluster.localcpuManagerReconcilePeriod: 0sevictionPressureTransitionPeriod: 0sfeatureGates: AllAlpha: false BoundServiceAccountTokenVolume: false EphemeralContainers: true RemoveSelfLink: false RotateKubeletServerCertificate: truefileCheckFrequency: 0shealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 0simageMinimumGCAge: 0skind: KubeletConfigurationkubeletCgroups: /system.slice/kubelet.servicelogging: {}nodeStatusReportFrequency: 0snodeStatusUpdateFrequency: 0srotateCertificates: trueruntimeRequestTimeout: 0sshutdownGracePeriod: 0sshutdownGracePeriodCriticalPods: 0sstaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 0ssyncFrequency: 0svolumeStatsAggPeriod: 0s 此时, 我们修改优雅关机的相关参数,比如: 12shutdownGracePeriod: 30s shutdownGracePeriodCriticalPods: 10s 在此节点上重启kubelet 123sudo systemctl daemon-reload sudo systemctl stop kubeletsudo systemctl start kubelet 此时我们修改就完成了 2.1 查看新配置是否生效登录到一个有kubenetes 集群访问权限的节点, 比如一个master节点, 然后开启kube proxy 12test@master-0-01:~&gt; kubectl proxy --api-prefix=/ &amp;[1] 6171 使用查询接口来查看 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114test@director-0-01:~&gt; curl http://127.0.0.1:8001/api/v1/nodes/poolalpha-worker-0-01/proxy/configz|jq % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 2520 0 2520 0 0 7522 0 --:--:-- --:--:-- --:--:-- 7522{ &quot;kubeletconfig&quot;: { &quot;enableServer&quot;: true, &quot;staticPodPath&quot;: &quot;/etc/kubernetes/manifests&quot;, &quot;syncFrequency&quot;: &quot;1m0s&quot;, &quot;fileCheckFrequency&quot;: &quot;20s&quot;, &quot;httpCheckFrequency&quot;: &quot;20s&quot;, &quot;address&quot;: &quot;0.0.0.0&quot;, &quot;port&quot;: 10250, &quot;tlsCertFile&quot;: &quot;/var/lib/kubelet/pki/kubelet.crt&quot;, &quot;tlsPrivateKeyFile&quot;: &quot;/var/lib/kubelet/pki/kubelet.key&quot;, &quot;rotateCertificates&quot;: true, &quot;authentication&quot;: { &quot;x509&quot;: { &quot;clientCAFile&quot;: &quot;/etc/kubernetes/pki/ca.crt&quot; }, &quot;webhook&quot;: { &quot;enabled&quot;: true, &quot;cacheTTL&quot;: &quot;2m0s&quot; }, &quot;anonymous&quot;: { &quot;enabled&quot;: false } }, &quot;authorization&quot;: { &quot;mode&quot;: &quot;Webhook&quot;, &quot;webhook&quot;: { &quot;cacheAuthorizedTTL&quot;: &quot;5m0s&quot;, &quot;cacheUnauthorizedTTL&quot;: &quot;30s&quot; } }, &quot;registryPullQPS&quot;: 5, &quot;registryBurst&quot;: 10, &quot;eventRecordQPS&quot;: 5, &quot;eventBurst&quot;: 10, &quot;enableDebuggingHandlers&quot;: true, &quot;healthzPort&quot;: 10248, &quot;healthzBindAddress&quot;: &quot;127.0.0.1&quot;, &quot;oomScoreAdj&quot;: -999, &quot;clusterDomain&quot;: &quot;cluster.local&quot;, &quot;clusterDNS&quot;: [ &quot;169.254.20.10&quot; ], &quot;streamingConnectionIdleTimeout&quot;: &quot;4h0m0s&quot;, &quot;nodeStatusUpdateFrequency&quot;: &quot;10s&quot;, &quot;nodeStatusReportFrequency&quot;: &quot;5m0s&quot;, &quot;nodeLeaseDurationSeconds&quot;: 40, &quot;imageMinimumGCAge&quot;: &quot;2m0s&quot;, &quot;imageGCHighThresholdPercent&quot;: 85, &quot;imageGCLowThresholdPercent&quot;: 80, &quot;volumeStatsAggPeriod&quot;: &quot;1m0s&quot;, &quot;kubeletCgroups&quot;: &quot;/system.slice/kubelet.service&quot;, &quot;cgroupsPerQOS&quot;: true, &quot;cgroupDriver&quot;: &quot;systemd&quot;, &quot;cpuManagerPolicy&quot;: &quot;none&quot;, &quot;cpuManagerReconcilePeriod&quot;: &quot;10s&quot;, &quot;memoryManagerPolicy&quot;: &quot;None&quot;, &quot;topologyManagerPolicy&quot;: &quot;none&quot;, &quot;topologyManagerScope&quot;: &quot;container&quot;, &quot;runtimeRequestTimeout&quot;: &quot;15m0s&quot;, &quot;hairpinMode&quot;: &quot;promiscuous-bridge&quot;, &quot;maxPods&quot;: 110, &quot;podPidsLimit&quot;: -1, &quot;resolvConf&quot;: &quot;/etc/resolv.conf&quot;, &quot;cpuCFSQuota&quot;: true, &quot;cpuCFSQuotaPeriod&quot;: &quot;100ms&quot;, &quot;nodeStatusMaxImages&quot;: 50, &quot;maxOpenFiles&quot;: 1000000, &quot;contentType&quot;: &quot;application/vnd.kubernetes.protobuf&quot;, &quot;kubeAPIQPS&quot;: 5, &quot;kubeAPIBurst&quot;: 10, &quot;serializeImagePulls&quot;: true, &quot;evictionHard&quot;: { &quot;imagefs.available&quot;: &quot;15%&quot;, &quot;memory.available&quot;: &quot;100Mi&quot;, &quot;nodefs.available&quot;: &quot;10%&quot;, &quot;nodefs.inodesFree&quot;: &quot;5%&quot; }, &quot;evictionPressureTransitionPeriod&quot;: &quot;5m0s&quot;, &quot;enableControllerAttachDetach&quot;: true, &quot;makeIPTablesUtilChains&quot;: true, &quot;iptablesMasqueradeBit&quot;: 14, &quot;iptablesDropBit&quot;: 15, &quot;featureGates&quot;: { &quot;AllAlpha&quot;: false, &quot;BoundServiceAccountTokenVolume&quot;: false, &quot;EphemeralContainers&quot;: true, &quot;RemoveSelfLink&quot;: false, &quot;RotateKubeletServerCertificate&quot;: true }, &quot;failSwapOn&quot;: true, &quot;containerLogMaxSize&quot;: &quot;10Mi&quot;, &quot;containerLogMaxFiles&quot;: 5, &quot;configMapAndSecretChangeDetectionStrategy&quot;: &quot;Watch&quot;, &quot;enforceNodeAllocatable&quot;: [ &quot;pods&quot; ], &quot;volumePluginDir&quot;: &quot;/usr/libexec/kubernetes/kubelet-plugins/volume/exec/&quot;, &quot;logging&quot;: { &quot;format&quot;: &quot;text&quot; }, &quot;enableSystemLogHandler&quot;: true, ################看这里#################### &quot;shutdownGracePeriod&quot;: &quot;30s&quot;, &quot;shutdownGracePeriodCriticalPods&quot;: &quot;10s&quot;, ######################################## &quot;enableProfilingHandler&quot;: true, &quot;enableDebugFlagsHandler&quot;: true }} 这里使用jq 使得生成的json更加可读 3. 结语上面修改kubelet参数只是一个例子, 修改的方法很多, 但查看其实际配置建议还是采用此接口","link":"/2021/07/26/kubernetes/how_to_check_kubelet_of_node/"},{"title":"[kubernetes] kustomize 入门例子","text":"0. 简介kustomize 形如customize, 也就是k8s+customize, 它是kube资源描述文件的一种抽象工具. 官方地址: https://kustomize.io/ 1. 为什么会用 kustomizekustomize它作用类似Helm, 它的实现方式和helm不同, helm是基于模版, 而kustomize是基于overlay 引擎. 通俗讲就是说, 你写一个普通的k8s资源描述yaml文件, 那么所有东西都是写死的, 此时如果你有helm, 你可以把部分值写成变量模版, 达到灵活的目的. 而kustomize更像是代码式的声明将,基代码和定制化代码重叠, 达到灵活的目的, 有时我们的配置比较简单且固定, 此时, 我们更希望一种快捷易于操作的方式, 那么此时就可以考虑kustomize, 当然这不代表kustomize只能写简单的东西. 我们来看一下需求是怎么来的: 假设我们有一个pod.yaml: 1234567891011# pod.yaml contentsapiVersion: v1kind: Podmetadata: name: myapp-pod labels: app: myappspec: containers: - name: web image: web:v.1.0 一切看起来都很好, 突然有一天, 老板说, 我们客户给了一个新环境, 我们得再部署一套应用到名叫XINHUANJING的新环境, 此时你可能会说, 好办, 看我粘贴复制大法, 只需改点点配置. 1234567891011# pod.yaml contentsapiVersion: v1kind: Podmetadata: name: XINHUANJING-myapp-pod labels: app: myappspec: containers: - name: web image: web:v.1.0 过了两天经理又来找你说, 我们的产品升级, 原来的k8s资源库也要更新, 把web app升级到2.0, 那么你可以继续粘贴复制, so easy! 1234567891011# pod.yaml contentsapiVersion: v1kind: Podmetadata: name: XINHUANJING-myapp-pod labels: app: myappspec: containers: - name: web image: web:v.2.0 等等, 我还需要更新客户环境的描述文件夹, 粘贴复制 … 接下来的日子, 你发现随着业务量的增加, 客户的跟进, 乱七八糟deployment, statfulset … 越来越多, 客户要的也不止改个名字那么简单, 此时粘贴复制好像就没那么好用了. 此时, 你便需要一个可以一通百通的方式来管理你的配置, 那么kustomize 和helm 就可以帮忙了. 2. 入门案例我们有两套环境, 我们需要分别在不同的开发环境(dev, prod)中, 配置不同的应用名 2.1 基代码123456789101112131415161718192021222324252627//创建一个 基代码文件夹test@master-0-test01:~&gt; mkdir base//创建基代码test@master-0-test01:~&gt; cd base/test@master-0-test01:~/base&gt; lskustomization.yaml pod.yaml//声明资源文件test@master-0-test01:~/base&gt; cat kustomization.yaml # kustomization.yaml contentsresources:- pod.yaml//资源pod文件, 可以是各种k8s复杂的资源, deployment, resfulset, cm ...test@master-0-test01:~/base&gt; cat pod.yaml # pod.yaml contentsapiVersion: v1kind: Podmetadata: name: myapp-pod labels: app: myappspec: containers: - name: nginx image: nginx:latest 此时, 你可以看出, 这个时候, 如果你直接kubectl apply 那么它会直接创建一个pod. 接下来我们来根据不同的环境改变这个pod 2.2 写各个环境的特殊配置假设我们有两个环境, 一个是开发环境 dev, 一个是 prod 环境, 我们希望 dev 环境的 pod 会标识出来 dev, 我们希望 prod 环境的 pod 会标识出来 prod 123456789101112131415161718192021test@master-0-test01:~&gt; mkdir overlaystest@master-0-test01:~&gt; cd overlays/test@master-0-test01:~/overlays&gt; lsdev prodtest@master-0-test01:~/overlays&gt; ls dev/kustomization.yamltest@master-0-test01:~/overlays&gt; ls prod/kustomization.yaml//dev 环境的描述文件, 此时使用了kustomize的 namePrefix 标签test@master-0-test01:~/overlays&gt; cat dev/kustomization.yaml resources:- ../../basenamePrefix: dev-//prod 环境的描述文件test@master-0-test01:~/overlays&gt; cat prod/kustomization.yaml resources:- ../../basenamePrefix: prod- 可以看出, 描述文件使用了资源 ../../base, 这里类似我们写代码的import, 而且是个静态的import 2.2 写一个总的入口文件这时我们想看看生成的每个环境的描述文件是否正确, 此时我们需要在工作目录的根目录下创建一个总括的 kustomization.yaml 12345test@master-0-test01:~&gt; cat kustomization.yaml resources:- ./overlays/dev- ./overlays/prodnamePrefix: my-name- 可以看到, 我们的resources引入了 刚才我们创建的两个环境. 再看看此时的目录结构(我没装tree) 12345678test@master-0-test01:~&gt; pwd/home/testtest@master-0-test01:~&gt; lsbase kustomization.yaml overlaystest@master-0-test01:~&gt; ls basekustomization.yaml pod.yamltest@master-0-test01:~&gt; ls overlays/dev prod 2.3 测试执行 kustomize build &lt;path-to-root-kustomization.yaml&gt; 会生成预览文件(和helm 很类似), 可以看出我们的pod名当前前缀都已经变化 12345678910111213141516171819202122232425// 如果你的集群没有kustomize, 可以下载: // curl -s &quot;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh&quot; | bashtest@master-0-test01:~&gt; kustomize buildapiVersion: v1kind: Podmetadata: labels: app: myapp name: my-name-dev-myapp-podspec: containers: - image: nginx:latest name: nginx---apiVersion: v1kind: Podmetadata: labels: app: myapp name: my-name-prod-myapp-podspec: containers: - image: nginx:latest name: nginx 2.4 创建pod123test@master-0-test01:~&gt; kubectl apply -k .pod/my-name-dev-myapp-pod createdpod/my-name-prod-myapp-pod created 3. 总结本例抛砖引玉, 实际生成也不可能只是个pod, 那么kustomize也提供了各种相关标签供大家使用: https://kubernetes.io/zh/docs/tasks/manage-kubernetes-objects/kustomization/","link":"/2021/07/12/kubernetes/how_to_use_kustomize/"},{"title":"[kubernetes]什么是overlay network","text":"概述 overlay network 这个概念是在接触kubernetes后常常遇到的, 比如pod之间. 它其实就是解决了一个问题,就是, 不同宿主机之间, 要想pod之间能直接通信, 那么就必须通过某种方法来达到目的 overlay network就是这么一个网络, 和它的名称一样, 是和我们直观的网络不在同一层的网络. 图示打个比方, 有两个宿主机172.10.0.4 和 192.168.1.5, 很明显他们不在同一网段, 不能直接通信, 两者之间用一个路由器链接. 那么此时假设, 两个机器上各有一个pod, 10.0.0.3和10.0.0.4, 而我们需要这两个pod之间直接通信的话, 那么这两个pod之间我们架设的网络, 就是一个overlay 网络, 也就是凌驾在节点自身网络之上的另一个网络, 一般就是将pod数据包再次封装成节点的数据包, 然后转发, 或者是建立隧道等等, 比如上面的例子的图示, 这里的Overlay 可以是个隧道, 可以是vpn, 可以是封装后的包通过路由 等等…1234567891011+---------------+ +---------------+| | | || +-------+ | | +-------+ || | pod1 |====[Overylay]====| pod2 | || +-------+ | | +-------+ || 10.0.0.3 | | 10.0.0.4 || | | |+---------------+ +---------------+ 172.10.0.4 192.168.1.5 | | |===========[Router]=========|","link":"/2021/08/09/kubernetes/what_is_overlay_network/"},{"title":"[openstack] 如何开关vm及类似操作命令","text":"往往这些操作是需要特定权限的, 确保操作时你拥有操作权限 1. 暂停/取消暂停 这种状态下VM的状态会保存到RAM中, CUP则不会, 取消后会继续暂停前的状态12$ openstack server pause myInstance$ openstack server unpause myInstance 2. 挂起/取消挂起 这种状态类似物理机休眠, 状态会保存在文件中, CUP和RAM会被释放出来12$ openstack server pause myInstance$ openstack server resume myInstance 3. 开机/关机12$ openstack server start myInstance$ openstack server stop myInstance 4. 检查机器状态4.1 简单查看, 即从所有实例中找目标机器123456$ openstack server list | grep myInstance+--------------------------------------+-------------+---------+----------------------------------------------------------------------------------------------| ID | Name | Status | Networks +--------------------------------------+-------------+---------+----------------------------------------------------------------------------------------------| 27b48f01-aed0-47e6-b3ab-c22e1d7064f4 | master-01 | SHUTOFF | internal-net-01=10.0.10.75, fd00::17:1111::2f7 .... 4.2 查看详细信息12345678910111213141516171819$ openstack server show master-01+-------------------------------------+------------------------------------------------------------------+| Field | Value |+-------------------------------------+------------------------------------------------------------------+| OS-DCF:diskConfig | MANUAL || OS-EXT-AZ:availability_zone | nova || accessIPv4 | || accessIPv6 | || addresses | internal-net-01=10.0.10.75, fd00::17:1111::2f7 || config_drive | True || created | 2021-07-08T08:48:53Z || flavor | large || id | f4bbcae1-5e7a-4b72-929a-f3fdbd06fdb9 || image | N/A (booted from volume) || key_name | None || name | master-01 || progress | 0 |... ...+-------------------------------------+------------------------------------------------------------------+ 官方的server操作文档(pike)","link":"/2021/07/02/openstack/openstack_server_status_operation_20210708/"},{"title":"runc 入门使用","text":"runc的个人理解结合日常应用, 我个人的理解就是一个运行于docker和操作系统之间的中间件, 也就是说docker 通过调用runc, 达到对容器的操作, 也就是说docker的所有操作也可以通过直接操作runc完成:docker —-&gt; runc —–&gt; os 测试环境假设此时, 我们有一个容器, 这里我们需要新建一个rootfs文件夹, runc也就是使用此文件内的文件为容器内部代码, 如果你观察, 其实就是docker 镜像内部文件 123#解包一个本地容器内容mkdir rootfssudo docker export $(sudo docker create busybox:1.34.1)|tar -C ./rootfs -xvf - 然后生成runc配置文件config.json, 配合上面的rootfs内部文件, 就可以用runc 启动容器了 1runc spec 此时会生成一个config.json, 如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180{ &quot;ociVersion&quot;: &quot;1.0.2-dev&quot;, &quot;process&quot;: { &quot;terminal&quot;: true, &quot;user&quot;: { &quot;uid&quot;: 0, &quot;gid&quot;: 0 }, &quot;args&quot;: [ &quot;sh&quot; ], &quot;env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;, &quot;TERM=xterm&quot; ], &quot;cwd&quot;: &quot;/&quot;, &quot;capabilities&quot;: { &quot;bounding&quot;: [ &quot;CAP_AUDIT_WRITE&quot;, &quot;CAP_KILL&quot;, &quot;CAP_NET_BIND_SERVICE&quot; ], &quot;effective&quot;: [ &quot;CAP_AUDIT_WRITE&quot;, &quot;CAP_KILL&quot;, &quot;CAP_NET_BIND_SERVICE&quot; ], &quot;inheritable&quot;: [ &quot;CAP_AUDIT_WRITE&quot;, &quot;CAP_KILL&quot;, &quot;CAP_NET_BIND_SERVICE&quot; ], &quot;permitted&quot;: [ &quot;CAP_AUDIT_WRITE&quot;, &quot;CAP_KILL&quot;, &quot;CAP_NET_BIND_SERVICE&quot; ], &quot;ambient&quot;: [ &quot;CAP_AUDIT_WRITE&quot;, &quot;CAP_KILL&quot;, &quot;CAP_NET_BIND_SERVICE&quot; ] }, &quot;rlimits&quot;: [ { &quot;type&quot;: &quot;RLIMIT_NOFILE&quot;, &quot;hard&quot;: 1024, &quot;soft&quot;: 1024 } ], &quot;noNewPrivileges&quot;: true }, &quot;root&quot;: { &quot;path&quot;: &quot;rootfs&quot;, &quot;readonly&quot;: true }, &quot;hostname&quot;: &quot;runc&quot;, &quot;mounts&quot;: [ { &quot;destination&quot;: &quot;/proc&quot;, &quot;type&quot;: &quot;proc&quot;, &quot;source&quot;: &quot;proc&quot; }, { &quot;destination&quot;: &quot;/dev&quot;, &quot;type&quot;: &quot;tmpfs&quot;, &quot;source&quot;: &quot;tmpfs&quot;, &quot;options&quot;: [ &quot;nosuid&quot;, &quot;strictatime&quot;, &quot;mode=755&quot;, &quot;size=65536k&quot; ] }, { &quot;destination&quot;: &quot;/dev/pts&quot;, &quot;type&quot;: &quot;devpts&quot;, &quot;source&quot;: &quot;devpts&quot;, &quot;options&quot;: [ &quot;nosuid&quot;, &quot;noexec&quot;, &quot;newinstance&quot;, &quot;ptmxmode=0666&quot;, &quot;mode=0620&quot;, &quot;gid=5&quot; ] }, { &quot;destination&quot;: &quot;/dev/shm&quot;, &quot;type&quot;: &quot;tmpfs&quot;, &quot;source&quot;: &quot;shm&quot;, &quot;options&quot;: [ &quot;nosuid&quot;, &quot;noexec&quot;, &quot;nodev&quot;, &quot;mode=1777&quot;, &quot;size=65536k&quot; ] }, { &quot;destination&quot;: &quot;/dev/mqueue&quot;, &quot;type&quot;: &quot;mqueue&quot;, &quot;source&quot;: &quot;mqueue&quot;, &quot;options&quot;: [ &quot;nosuid&quot;, &quot;noexec&quot;, &quot;nodev&quot; ] }, { &quot;destination&quot;: &quot;/sys&quot;, &quot;type&quot;: &quot;sysfs&quot;, &quot;source&quot;: &quot;sysfs&quot;, &quot;options&quot;: [ &quot;nosuid&quot;, &quot;noexec&quot;, &quot;nodev&quot;, &quot;ro&quot; ] }, { &quot;destination&quot;: &quot;/sys/fs/cgroup&quot;, &quot;type&quot;: &quot;cgroup&quot;, &quot;source&quot;: &quot;cgroup&quot;, &quot;options&quot;: [ &quot;nosuid&quot;, &quot;noexec&quot;, &quot;nodev&quot;, &quot;relatime&quot;, &quot;ro&quot; ] } ], &quot;linux&quot;: { &quot;resources&quot;: { &quot;devices&quot;: [ { &quot;allow&quot;: false, &quot;access&quot;: &quot;rwm&quot; } ] }, &quot;namespaces&quot;: [ { &quot;type&quot;: &quot;pid&quot; }, { &quot;type&quot;: &quot;network&quot; }, { &quot;type&quot;: &quot;ipc&quot; }, { &quot;type&quot;: &quot;uts&quot; }, { &quot;type&quot;: &quot;mount&quot; } ], &quot;maskedPaths&quot;: [ &quot;/proc/acpi&quot;, &quot;/proc/asound&quot;, &quot;/proc/kcore&quot;, &quot;/proc/keys&quot;, &quot;/proc/latency_stats&quot;, &quot;/proc/timer_list&quot;, &quot;/proc/timer_stats&quot;, &quot;/proc/sched_debug&quot;, &quot;/sys/firmware&quot;, &quot;/proc/scsi&quot; ], &quot;readonlyPaths&quot;: [ &quot;/proc/bus&quot;, &quot;/proc/fs&quot;, &quot;/proc/irq&quot;, &quot;/proc/sys&quot;, &quot;/proc/sysrq-trigger&quot; ] }} 启动容器并进入容器 123456789101112~/busybox&gt; sudo runc run test/ # / # / # lsbin dev etc home proc root sys tmp usr var/ # envSHLVL=1HOME=/rootTERM=xtermPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binPWD=// # 这里面的env显示, 实际我们可以通过confg.json中的env来进行更改, 比如我们加一个ABC=EFG, 那么生成的新的容器就会读取此环境变量 再切换终端, 看看运行时容器 123~&gt; sudo runc listID PID STATUS BUNDLE CREATED OWNERtest 35850 running /home/bx/busybox 2022-05-09T13:36:30.6041705Z root 配置文件的使用runc 在生成这个配置文件config.json后, 便会以这个文件为基准来创建容器, 这里也有很多常见的docker配置, 比如, env, mount等, 这里以mount为例, 我们在上面的配置中添加一个挂载点 1234567891011121314{ &quot;mounts&quot;: [ ... { &quot;destination&quot;: &quot;/app&quot;, &quot;type&quot;: &quot;bind&quot;, &quot;source&quot;: &quot;/home/test/runc/app&quot;, &quot;options&quot;: [ &quot;rbind&quot;,&quot;rw&quot; ] }, ... ]} 再次启动容器 1234~&gt; sudo runc run mytest~&gt; sudo runc listID PID STATUS BUNDLE CREATED OWNERmytest 107520 running /home/test/runc/alpine 2022-05-09T18:46:37.156872532Z root 进入容器查看挂载的文件 1234567~&gt; sudo runc exec -t mytest sh/ # lsapp bin dev etc home lib media mnt opt proc root run sbin srv sys tmp usr var/ # cd app//app # lstest.sh/app # 这个挂载点就对应了我们本地挂载的文件夹, 里面有个我们创建的test.sh 1234~/runc/app&gt; lstest.sh~runc/app&gt; pwd/home/test/runc/app 删除容器 关闭容器进程 1runc kill &lt;pid&gt; 删除容器 1runc delete &lt;pid&gt;","link":"/2022/05/09/runc/runc-hello-md/"},{"title":"[Vagrant] 入门","text":"1.什么是vagrant具体定义大家可以参见官方网站: https://www.vagrantup.com/. 那么大白话vagrant 到底是干嘛的, 它和docker有点像, 甚至部分操作用过docker 的人都会觉得似曾相识, 我们知道, docker 是来管理container的, 而vagrant是来管理虚拟机的, 比如你有个tomcat, 这个tomcat可以运行在docker容器上, 我们通过docker来管理这个容器, 而这个docker容器可以运行在VM上, 而vagrant可以是用来管理这个VM的, 当然, 这个只是一个宽泛的解释, 具体还是需要深入理解其本质. 2.如何工作vagrant 通过不同的provider 来对虚拟机软件的API进行调用, 例如 virtualbox, 那么vagrant 通过virtualbox provider 来操作VM. 也就省去了我们手动创建的麻烦 3. 安装 更多方式: https://www.vagrantup.com/downloads Linux/Ubuntu 123curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -sudo apt-add-repository &quot;deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main&quot;sudo apt-get update &amp;&amp; sudo apt-get install vagrant Windows这里注意，如果你使用virtualbox或者vmware, 并且运行你普通使用创建vm没问题，那么你只需要根据上面的链接安装vagrant, 如果你不要因为vagrant需要hyper-v, 所以你要打开自己的windows hyper-v, 传送门 然后根据上面的下载地址，下载安装 MacOS 12brew tap hashicorp/tapbrew install vagrant 验证是否已经安装 12$ vagrant --versionVagrant 2.2.17 4. 下载基础镜像这里我们以virtualbox 为例, 创建一个centos 7 的基础镜像， 那么我们需要现在vagrant上面下载: https://app.vagrantup.com/centos/boxes/7 点击这个镜像，可以看到我们的安装方式有vagrant file 和 new. 我们使用文件的方式, 本地新建一个文件夹用于vagrant, 然后创建这样一个文件，并执行安装. 123456789101112131415161718192021222324252627282930//创建Vagrantfiletest@agv1vvp2:~/vagrant$ cat&gt;Vagrantfile Vagrant.configure(&quot;2&quot;) do |config| config.vm.box = &quot;centos/7&quot;end//安装test@1vvp2:~/vagrant$ vagrant up --provider=virtualbox --colorBringing machine 'default' up with 'virtualbox' provider...==&gt; default: Box 'centos/7' could not be found. Attempting to find and install... default: Box Provider: virtualbox default: Box Version: &gt;= 0==&gt; default: Loading metadata for box 'centos/7' default: URL: https://vagrantcloud.com/centos/7==&gt; default: Adding box 'centos/7' (v2004.01) for provider: virtualbox default: Downloading: https://vagrantcloud.com/centos/boxes/7/versions/2004.01/providers/virtualbox.boxDownload redirected to host: cloud.centos.org default: Calculating and comparing box checksum...==&gt; default: Successfully added box 'centos/7' (v2004.01) for 'virtualbox'!... ...//查看Vagrant 是否在运行test@agv1vvp2:~/vagrant$ vagrant statusCurrent machine states:default running (virtualbox)The VM is running. To stop this VM, you can run `vagrant halt` toshut it down forcefully, or you can run `vagrant suspend` to simplysuspend the virtual machine. In either case, to restart it again,simply run `vagrant up`. 如果下载速度过慢，可以采用科学上网，或者添加新的镜像源, 源自己找就可以了，网上一堆:1$ vagrant box add centos7 &lt;image link&gt; 5. ssh到虚拟机 最快的方式 12test@agv1vvp2:~/vagrant$ vagrant sshLast login: Wed Jul 14 13:02:41 2021 from 10.0.2.2 传统ssh 的方式 有时我们需要传统的方式来ssh，那么我们需要先查看ssh 配置 1234567891011test@agv1vvp2:~/vagrant$ vagrant ssh-configHost default HostName 127.0.0.1 User vagrant Port 2222 UserKnownHostsFile /dev/null StrictHostKeyChecking no PasswordAuthentication no IdentityFile /home/test/vagrant/.vagrant/machines/default/virtualbox/private_key IdentitiesOnly yes LogLevel FATAL 然后ssh 1ssh vagrant@127.0.0.1 -p 2222 -o LogLevel=FATAL -o Compression=yes -o DSAAuthentication=yes -o IdentitiesOnly=yes -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i /home/test/vagrant/.vagrant/machines/default/virtualbox/private_key 或者用配置文件的方式 123test@agv1vvp2:~/vagrant$ vagrant ssh-config &gt; vagrant-sshtest@agv1vvp2:~/vagrant$ ssh -F vagrant-ssh defaultLast login: Wed Jul 14 13:01:37 2021 from 10.0.2.2 6. 配置自己的虚拟机实际上, 这个Vagrantfile 就是一个Ruby 脚本, 它本身会调用Vagrant的API, 首先观察这个文件 123Vagrant.configure(&quot;2&quot;) do |config| config.vm.box = &quot;centos/7&quot;end 可以看到, config.vm.box 这一行代码, 显而易见,这里就是指定了我们镜像的类型, 那么假设我们需要搭建多个节点, 我们同样可以扩展这个文件 12345678910111213141516171819Vagrant.configure(&quot;2&quot;) do |config| config.vm.box = &quot;centos/7&quot; //配置第一台vm, hostname为 server1 config.vm.define &quot;node1&quot; do |server1| server1.vm.hostname = &quot;server1&quot; end //配置第一台vm, hostname为 server2 config.vm.define &quot;node2&quot; do |server2| server2.vm.hostname = &quot;server2&quot; end //配置第一台vm, hostname为 server3 config.vm.define &quot;node3&quot; do |server3| server3.vm.hostname = &quot;server3&quot; endend 重新创建 123456789test@agv1vvp2:~/vagrant$ vagrant upBringing machine 'node1' up with 'virtualbox' provider...Bringing machine 'node2' up with 'virtualbox' provider...Bringing machine 'node3' up with 'virtualbox' provider...... ...//查看其中一台vmtest@agv1vvp2:~/vagrant$ vagrant ssh node1[vagrant@server1 ~]$ exit 此时, 如果你打开你的virtualbox, 可以看到自己创建的三个文件 如果需要更多配置, 可以参考官方文档: https://www.vagrantup.com/docs/vagrantfile 7. 常用的命令 开机 1vagrant up 关机 1vagrant halt 关机并删除 1vagrant destroy ssh连接到虚拟机 1vagrant ssh &lt;host_name&gt; 重新加载Vagrantfile 1vagrant reload","link":"/2021/07/14/vagrant/vagrant_for_beginners/"},{"title":"[博客搭建 0] - 新建并挂载自己的博客到github page","text":"0. 搭建个人博客原料github pages 这里, 由于我们的个人博客是运行中github上, 使用过得是github提供的免费的github pages, 所以github是必要的 Hugo Hugo是一个免费的go语言博客框架, 会帮助我们生成静态的html及相关脚本, 类似Hexo, 本教程使用的是Hugo框架 1. 安装HUGO Windows 打开Powershell, 右键管理员权限, 使用choco安装 1$ choco install hugo -confirm Linux 打开Terminal, 使用snap安装 1$ sudo snap install hugo 更多安装方式, 请参考官方文档: https://gohugo.io/getting-started/installing/ 2. 创建一个hugo github page2.1 创建git page 的 repositoryRepository 就是一个普通的git 项目, git page 是通过解析项目来加载我们的静态网页文件, 小白的话, 就不必关心它是什么了 登录 https://github.com/, 点击 “Sign Up”, 根据提示完成github的注册 登录自己的github 创建一个repository, 注意这里需要填写和你用户名一致的repo名, 再跟上.github.io 选择public, 然后点击create repository 接下来你会看到一个Git操作指引, 这里先不要管, 我们先做后续的步骤 2.2 安装git由于生成的git操作指引是需要安装git的, 那么我们需要先安装它, 安装信息: https://git-scm.com/downloads 2.3 创建一个本地hugo 网页库例如, 我在 ~/tmp/blog 下创建, 那么执行 hugo new site . 12345678910111213test@1vvp2:~/tmp/blog$ hugo new site .Congratulations! Your new Hugo site is created in /home/tmp/tmp/blog.Just a few more steps and you're ready to go:1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/ or create your own with the &quot;hugo new theme &lt;THEMENAME&gt;&quot; command.2. Perhaps you want to add some content. You can add single files with &quot;hugo new &lt;SECTIONNAME&gt;/&lt;FILENAME&gt;.&lt;FORMAT&gt;&quot;.3. Start the built-in live server via &quot;hugo server&quot;.Visit https://gohugo.io/ for quickstart guide and full documentation. 此时如果你查看当前的文件夹, 会发现hugo 已经创建了所有需要的文件 12test@1vvp2:~/tmp/blog$ lsarchetypes config.toml content data layouts static themes 测试一下网页, 会提示本地网站运行在 http://localhost:1313/ 1test@1vvp2:~/tmp/blog$ hugo server 浏览器输入 http://localhost:1313/ 发现是空白页, 但是没有报错, 证明hugo运行正常, 空白的原因是我们没有加载主题 2.3 使用Hugo主题登录 https://themes.gohugo.io/tags/blog/ 选择一款自己喜欢的主题, 一般主题分Blog 和 个人主页, 如果是想突出个人, 就选择 个人主页, 如果是记录性质的, 选择 Blog这里以cactus 为例 1234567891011tmp@e1vvp2:~/tmp/blog$ cd ~/tmp/blog//克隆主题到本地的/themetmp@e1vvp2:~/tmp/blog$ git clone https://github.com/monkeyWzr/hugo-theme-cactus.git themes/cactus... ...tmp@e1vvp2:~/tmp/blog/themes$ cd cactus/tmp@e1vvp2:~/tmp/blog/themes/cactus$ lsassets exampleSite images layouts LICENSE README.md static theme.toml//将模版网页应用于本地tmp@e1vvp2:~/tmp/blog/themes/cactus$ cp exampleSite/* ../../ 此时, 再次运行 1hugo server 登录 http://localhost:1313/, 你会发现现在的页面已经变了 2.4 创建新的页面在hugo 的本地网页代码库中, content文件夹下创建文件夹 posts, 然后创建一个 test.md 的markdown文件输入如下内容 1234567---title: &quot;你好&quot;tags: [&quot;hello&quot;]categories: [&quot;hello&quot;]---你好 此时再次查看 http://localhost:1313/, 你会发现你的页面已经出现了 通过 Ctrl+c 来停止正在运行的网页 2.5 将自己的页面和github page 连起来这时我们再回头看看刚才github为我们创造的操作指南, 那么我们现在需要将本地的网页放在github上跑起来 生成静态页面 hugo -D12345678910111213141516171819202122test@1vvp2:~/tmp/blog$ hugo -DStart building sites … hugo v0.85.0+extended linux/amd64 BuildDate=2021-07-05T14:34:48ZWARNING: calling IsSet with unsupported type &quot;ptr&quot; (*hugolib.SiteInfo) will always return false. | EN -------------------+----- Pages | 13 Paginator pages | 0 Non-page files | 0 Static files | 59 Processed images | 0 Aliases | 3 Sitemaps | 1 Cleaned | 0 Total in 56 ms//此时会生成一个静态网页文件夹publictest@1vvp2:~/tmp/blog$ lsarchetypes config.toml content data deploy.sh layouts netlify.toml public resources static themes 推送自己的网页到远端12345678910111213141516171819202122232425// 进入public 文件夹, 执行初始化 gittmp@e1vvp2:~/tmp/blog/public$ git initInitialized empty Git repository in /home/tmp/tmp/blog/public/.git/// 给定远端的博客地址tmp@e1vvp2:~/tmp/blog/public$ git remote add origin git@github.com:JohnAndEthan/JohnAndEthan.github.io.git// 添加本地页面代码, 并提交tmp@e1vvp2:~/tmp/blog/public$ git add .tmp@e1vvp2:~/tmp/blog/public$ git commit -m &quot;我的博客&quot;[master (root-commit) 054cd26] 我的博客 78 files changed, 11636 insertions(+) // 提交到远端库tmp@e1vvp2:~/tmp/blog/public$ git branch -M maintmp@e1vvp2:~/tmp/blog/public$ git push -u origin mainCounting objects: 110, done.Delta compression using up to 8 threads.Compressing objects: 100% (100/100), done.Writing objects: 100% (110/110), 3.11 MiB | 1.53 MiB/s, done.Total 110 (delta 23), reused 0 (delta 0)remote: Resolving deltas: 100% (23/23), done.To github.com:JohnAndEthan/JohnAndEthan.github.io.git * [new branch] main -&gt; mainBranch 'main' set up to track remote branch 'main' from 'origin'. 2.6 修改 baseURL记得在 config.toml 中修改自己的baseURL 指向你的github page地址, 不然有可能页面不能加载主题 1baseURL = &quot;https://johnandethan.github.io/&quot; 3. 测试此时输入刚才我们 创建的repo地址, https://johnandethan.github.io/ 就可以看到我们的网页了 4. 遇到问题怎么办 如果是失误操作, 直接删掉你本地的文件夹, 和github 的repository 重来就行了 如果是其他问题, 一般多数是由于主题的使用, 建议仔细阅读主题的相关文档","link":"/2021/07/09/blog/hugo/how-to-create-blog/"},{"title":"[博客搭建 1] - 在goDaddy购买域名并绑定博客","text":"0. 域名(Domain name)是什么维基百科中定义: 网域名称（英语：Domain Name，简称：Domain），简称域名、网域，是由一串用点分隔的字符组成的互联网上某一台计算机或计算机组的名称，用于在数据传输时标识计算机的电子方位。域名可以说是一个IP地址的代称，目的是为了便于记忆后者。大白话就是说, 用一个好记的名字来代表ip地址. 类似别人问你在哪, 你会说”我在北京路”, 而不会说我在北纬41°24’12.2 … 1. 为什么博客要Domain name高端,大气,上档次 2. 在哪儿买Domain name这里就放两个常用的: 国内: https://wanwang.aliyun.com/domain/ 国外: https://www.godaddy.com/ 3. 实例 (goDaddy)由于我是用goDaddy, 我这里就直接用它来讲解 3.1 搜索并购买域名登录 https://hk.godaddy.com/ 实例并搜索你要的域名根据自己的需要选定服务, 例如这里的全方位保护项, 戴保护肯定安全, 不戴后果自负, 反正我不爱戴付账, 邮箱确认, 完毕 3.2 绑定自己的git page3.2.1 配置 A 记录先说概念, A(Address)记录: 顾名思义就是地址, 也就是说, 假设你访问 google.com, 那么请求发出去后, 实际是访问google.com 的IP地址, 就是这个地址, 当然真正的访问过程要复杂得多, 这里简单理解即可, 配置位置点击自己的域名, 既可以开始配置, 此时有可能要求你邮箱确认, 你确认一下拖到最下面, 进入配置页面点击编辑按钮, 编辑A记录, 此时配置地址 185.199.108.153, 这个地址就是 git page 的 IP当前的ip地址包含: GIT PAGE IPS 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 如果你觉得填一个不保险, 点击 “加入” 按钮, 填写方法和图里面的一样添加 185.199.109.153 … 3.2.2 将git page 和 domain name 连起来登录自己的github page工程, 添加一个名为CNAME的文件CNAME文件内容就是你自己的域名 3.3 测试 3.4 http 添加 ssl 证书也就是 http -&gt; https, 这一步需要在你自己的git page 工程下, 勾选 “Enforce HTTPS”, 这里需要一定的时间, 因为github 需要生成ssl证书给你过一段时间后, 你便可以看到自己的博客协议已经是https, 不过, 这里有个证书信任问题, 我们可以后面在处理 4. 结论没结论","link":"/2021/07/05/blog/hugo/how-to-buy-a-domain-name/"},{"title":"[jenkins]Intellij jenkins pipeline代码高亮设置","text":"问题在CICD的建设中, jenkins的pipeline代码一般是脚本的形式存在, 作为一个开发人员, 我们往往熟悉了idea给我们提供的代码高亮和验证功能, 那么是否可以实现代码高亮? 答案是可以, 但是并没有我们写传统java, Python等代码时那么完善, 不过总被没有强 方法配置pipeline代码让groovy可以识别在Intellij 的setting中, 找到File types, 然后按下图让Intellij识别你的Jenkinsfile为groovy文件 配置pipeline的语法声明描述文件假设你的项目是如下模样,Jenkinsfile在根目录:然后在你的jenkins pipeline 上找到 IntelliJ IDEA GDSL, 并复制它最后在你的项目src中, 也就是源文件中创建一个jenkins.gdsl此时就完成了配置 效果再次查看自己的Jenkinsfile 已经有了部分提示正如之前所说, 这个地方还不完善, 网上也有其他的gdsl文件, 大家可以关注 这个issue","link":"/2021/10/22/cicd/jenkins/how-to-highlight-jenkins-pipeline-code/"},{"title":"Jenkins pipeline 中的环境变量","text":"这里Jenkins pipeline中的环境变量就是操作系统的环境变量, 和你在linux 中执行export一个变量是一样的, 其类型为String，不同的是，这个变量是位于这个pipeline生命周期内,也就是说pipeline结束,这个env就没有了 如何查看Jenkins 的环境变量直接通过访问master节点的env-vars.html直接通过访问master节点的env-vars.html, 例如 http://&lt;HOST&gt;/env-vars.html/, 你可以看到所有当前的环境变量, 但是这些环境变量只是jenkins此时预制的环境变量 通过 printenv 查看编辑Jenkinsfile脚本, 使用printenv.例子: 1234567891011pipeline { agent { label &quot;master&quot; } stages { stage(&quot;Env Variables&quot;) { steps { sh &quot;printenv&quot; } } }} 定义一个环境变量编辑Jenkinsfile脚本, 例如定义一个 MY_ENV: 12345678910111213141516pipeline { agent { label &quot;master&quot; } environment{ MY_ENV=&quot;TEST ENV&quot; } //这里打印刚才定义的变量及 打印所有变量 stages { stage(&quot;Env Variables&quot;) { steps { sh &quot;my test env: ${MY_ENV}&quot; sh &quot;printenv&quot; } } }} 运行结果: 12345678910111213141516[Pipeline] sh+ echo TEST ENVmy test env: TEST ENV[Pipeline] sh+ printenvJENKINS_HOME=/var/jenkins_homeGIT_PREVIOUS_SUCCESSFUL_COMMIT=08d9a22a56672004193417d7e1c0ee0299c5b3a1JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimentalCI=trueRUN_CHANGES_DISPLAY_URL=http://192.168.60.11:8080/job/test_my_pipeline/22/display/redirect?page=changesHOSTNAME=f350d2b99e0aNODE_LABELS=masterHUDSON_URL=http://192.168.60.11:8080/GIT_COMMIT=df2e37a99a4f7a96793614d0c17f2912b4967ca1SHLVL=0... ... 重写环境变量的值此时两种情况: 使用 environment{} 定义的环境变量 非 environment{} 环境变量 看下面的例子: 1234567891011121314151617181920212223242526272829303132333435pipeline { agent { label &quot;master&quot; } //使用environment{}定义MY_ENV environment{ MY_ENV=&quot;TEST ENV&quot; } stages { stage(&quot;Env Variables&quot;) { steps { sh &quot;my test env: ${MY_ENV}&quot; sh &quot;printenv&quot; } } stage('Set new env') { steps{ script { //不使用environment{}定义 env.MY_NEW_ENV = &quot;foo&quot; //尝试修改MY_ENV env.MY_ENV = &quot;bar&quot; sh &quot;echo MY_NEW_ENV: ${MY_NEW_ENV}, MY_ENV: ${MY_ENV}&quot; env.MY_NEW_ENV = &quot;foofoofoo&quot; sh &quot;echo MY_NEW_ENV: ${MY_NEW_ENV}, MY_ENV: ${MY_ENV}&quot; } //使用withEnv 来修改MY_ENV withEnv([&quot;MY_ENV=bar&quot;]) { echo &quot;MY_ENV = ${env.MY_ENV}&quot; } } } }} 运行: 123456789101112131415... ...[Pipeline] sh+ echo MY_NEW_ENV: foo, MY_ENV: TEST ENVMY_NEW_ENV: foo, MY_ENV: TEST ENV[Pipeline] sh+ echo MY_NEW_ENV: foofoofoo, MY_ENV: TEST ENVMY_NEW_ENV: foofoofoo, MY_ENV: TEST ENV[Pipeline] }[Pipeline] // script[Pipeline] withEnv[Pipeline] {[Pipeline] echoMY_ENV = bar[Pipeline] }... ... 结论上面的输出可以看出: 普通直接定义的环境变量只需要 env. 赋值的形式即可 如果是使用 environment{} 定义的环境变量, 需要使用 withEnv([])方法, 不然是不能修改成功的.","link":"/2021/10/22/cicd/jenkins/jenkins-environment-vras/"},{"title":"jenkins pipeline加载groovy脚本","text":"需求有时我们希望将jenkins pipeline的代码模块化, 比如将一些公共常用的方法写在一个groovy文件中然后在Jenkinsfile 调用, 那么这时, 我们可以使用Jenkins Shared Libraries, 但有时我们的代码量不大时, 可以考虑直接加载脚本 插件 Pipeline: Groovy首先安装Pipeline: Groovy插件, 详见Pipeline: Groovy 如何使用假设此时你有一个工程是这样: 1234567project-with-tests$ tree -L 1.├── Jenkinsfile├── logs.groovy├── pom.xml├── src└── target 而你自己定义了一个logs.groovy 来配置一些日志相关的操作, 例如 123456//logs.groovyvoid echoTest() { echo &quot;test&quot;}return this 注意, 这里最后有一个 return this 这是非常重要的, 那么如何使用呢, 我们往下看 12345678910111213141516171819202122232425262728293031//Jenkinsfileimport hudson.model.*def logpipeline { agent { label &quot;master&quot; } stages { stage('build') { steps { script { log = load &quot;${WORKSPACE}/logs.groovy&quot; log.echoTest() } } } stage('Result') { steps { junit '**/target/surefire-reports/TEST-*.xml' archiveArtifacts 'target/*.jar' } } } post { always { script { echo &quot;DONE&quot; } } }} 注意这里的 log = load &quot;${WORKSPACE}/logs.groovy&quot; 这里根据需要调整脚本位置即可 总结这里注意两点 注意被调用的groovy脚本, 最后需要返回(retrun this) 注意 load 脚本时, 根据脚本路径调整其位置 这种方法适合小型项目,不适合大型项目, 因为依旧会随着代码库的增大, 不易读的代码增多, 大型项目应使用 Jenkins Shared Libraries","link":"/2022/03/08/cicd/jenkins/jenkins-load-groovy-script-md/"},{"title":"[Jenkins Job DSL]jenkins pipeline 编写共享库","text":"Jenkins Shared Libraries使用Jenkins pipeline时, 我们有时不希望代码过于混乱, 代码过于重复,比如一个公用方法, 公用类我们希望它是编写一次, 然后被共享, 但是在Job DSL Plugin 1.60, 以前, 我们可以通过注入共享的groovy文件来达到目的, 但是新版本由于安全问题 Script-Security, 这种注入的方法被关闭, 如若没有管理员权限开放, 这种方法是不可以使用了: To avoid loading arbitrary code from the workspace without approval, the script directory is not added to the classpath and additional classpath entries are not supported when security is enabled. Thus importing classes from the workspace is not possible and the “Additional Classpath” option is not available. 如何使用和扩展 Shared Libraries官网介绍的比较清楚: 传送门 接着上面官网的例子, 实际应用中需要注意的几个点: 目录结构这个结构中src 可以理解为java中的src, vars可以理解为你要写的pipeline step调用方法, 比如我们可以将一些公用的类, 方法放在src, 比如下图中Bar.groovy, 然后, 你计划写一个 sayHi() 的step中的方法, 那么可以在这里写一个sayHi.groovy, 这个sayHi.groovy 可以调用Bar.groovy这个类. 12345678910111213(root)+- src # Groovy source files| +- org| +- foo| +- Bar.groovy # for org.foo.Bar class+- vars| +- foo.groovy # for global 'foo' variable| +- foo.txt # help for 'foo' variable| +- sayHi.groovy +- resources # resource files (external libraries only)| +- org| +- foo| +- bar.json # static helper data for org.foo.Bar step 方法的写法紧接着上面的例子, 那么Bar.groovy和sayHi.groovy分别怎么写 123456//Bar.groovypackage org.fooclass Bar { String name} 1234567//sayHi.groovyimport org.foo.Studentdef call(String name = &quot;foo&quot;) { def bar = new Bar('name': name) echo &quot;Hello, ${bar.name}.&quot;} 这个地方最重要是注意, 包, 也就是”package” 和 “import”, 生产中, 有时会因为包产生类似 cannot resolve symbol 或者找不到包这些问题, 这也是你debug时候可能遇到的问题. 另外一个, 这个地方必须实现call()方法, 也就是脚本的入口 使用自己的公共文件我这里没有说 “使用自己的共享库”, 而是 “自己的公共文件”, 原因是, 你不用共享库同样可以达到共享的效果, 请往下看 classpath 的配置 Job DSL 1.60 插件版本前 这个版本前可以参考 https://github.com/sheehan/job-dsl-gradle-example, 这个例子包含是一个以gradle为管理工具的工程, 不是重点, 重点是需要和上面说的目录结构一致即可. 1.6 版本前, 我们可以使用Additional classpath, 比如, 这个工程中, 目录结构是: 1234567891011.├── src│ ├── jobs # DSL script files│ ├── main│ │ ├── groovy # support classes│ │ └── resources│ │ └── idea.gdsl # IDE support for IDEA│ ├── scripts # scripts to use with &quot;readFileFromWorkspace&quot;│ └── test│ └── groovy # specs└── build.gradle # build file 然后在自己jenkins UI Job界面上, BUILD下process Job DSLs 项中配置Additional classpath: src/main/groovy. 也就是让你的pipeline 脚本能找到这个.class文件. 这个地方个人理解, 因为jenkins pipeline类加载的方式和java, groovy 不太一样, 它有自己的顺序, 所以也就有了之前包的问题的存在. 即便如此, 你还是需要注意包的问题, 也就是 “import” 和 “package”. 假设你的additional path配置不好, 很有可能碰到一些问题. 之前就因为同时配置时错误, 导致src是一个普通存在的包, 所以, 绕道(因为没权限)测试发现我们需要”package src”, 引入时就是”import src.Myclass”. 总之,这里也就是我之前提到的不用共享库同样可以达到共享的效果. Job DSL 1.60 插件版本后 前面也说了, 1.6后, 你可以手动在全局配置 Configure Global Security, 中不勾选Enable script security for Job DSL scripts, 这样你就可以继续使用Additional classpath, 而不用上面的共享库. 但是不建议这么做, 显然如果有人恶意利用漏洞(CVE-2018-1000861), 那么老板和小姨子只能跑路了. 安全期间,还是在jenkins中配置自己的库, 具体参考官方文档, 一般快速配置就是在全局配置中配置 Global Pipeline Libraries, 配置自己的代码库即可, 同时也可以根据需要, 使用灵活的配置方法. 调用你可以在你的dsl job 中build Triggers 下 Pipeline选项卡中, 选择Pipeline script选项卡下测试 12345678910//调用共享库@Library('pipeline-library-demo')_// 你也可以直接引用共享库中的类// import com.wj.GlobalVarsstage('Demo') { echo 'Hello world' //调用 sayHi.groovy, sayHi中定义了call方法. sayHi 'test test test'} 开发框架这里有一个比较好的gradle管理, jenkins dsl 共享库插件, 但是年久失修, 不过能凑合用: 插件, 例子 参考 https://emilwypych.com/2018/04/15/jenkins-unable-resolve-class-utilities-myutilities/ https://github.com/jenkinsci/job-dsl-plugin/wiki/Script-Security Jenkins + Groovy脚本 = 高效✔✔ （纯干货） intellij","link":"/2021/11/10/cicd/jenkins/jenkins-sharedlib-tuto/"},{"title":"go 限制并行线程数","text":"go中的多线程go 中启动线程很方便, 只需要在函数前 写go 关键字即可 1234go func(i int) { log.Println(i) &lt;-ch}(100) 那么这里就有了问题, 如何限定多少个线程同时执行(或者说同批次)? 在生产环境, 为了避免并发数量过高, 引发: panic: too many concurrent operations on a single file or socket 那么我们需要解决这一问题 问题一, 如何保证所有线程都能结束 java中我们常使用 CountDownLatch, go中也有类似的工具 sync.WaitGroup, 它主要包含以下三个方法 123456// 添加屏障, delat 表示添加屏障的个数wg.Add(delat int)// 表示屏障-1wg.Done()// 表示阻塞等待, 等待屏障为0wg.Wait() 通俗的讲, 有点类似短跑比赛, 裁判就是这个WaitGroup, 假设10个选手参加比赛, 那么裁判就在自己的比赛板上写10个人的名字,wg.Add(10). 然后开始比赛, 每当一个选手通过终点, 那么裁判就按一下秒表wg.Done(), 也就是比赛的人数-1. 期间裁判会一直等待wg.Wait(), 直到10个人都跑完. 将上面的例子写成代码(这个代码不能保证线程同时开始, 不过这里不关心) 12345678910111213func TestWaitGroup(t *testing.T) { var wgp sync.WaitGroup for i := 0; i &lt; 10; i++ { wgp.Add(1) //模拟跑步 go func(i int) { defer wgp.Done() fmt.Println(&quot;Runner No.&quot;, strconv.Itoa(i), &quot; finished&quot;) Sleep(Second) }(i) } wgp.Wait()} 问题二, 如何保证同批次假设100个人比赛, 为了保证跑步每组10个, 我们应该是在跑步前, 让10个人都不要动(准确说是永远最多10人在跑道上起跑), 那么如何保证. 1234567891011121314func TestWaitGroup(t *testing.T) { var wgp sync.WaitGroup for i := 0; i &lt; 10; i++ { wgp.Add(1) // -------------&gt; 这里我们需要每十个一批阻塞 //模拟跑步 go func(i int) { defer wgp.Done() fmt.Println(&quot;Runner No.&quot;, strconv.Itoa(i), &quot; finished&quot;) Sleep(Second) }(i) } wgp.Wait()} 这里, 我们可以使用chan 12345678910111213141516171819202122232425262728293031323334package testimport ( &quot;fmt&quot; &quot;golang.org/x/sys/unix&quot; &quot;strconv&quot; &quot;sync&quot; &quot;testing&quot; . &quot;time&quot;)var wg sync.WaitGroup//一个size为10 的channelvar batchCh = make(chan struct{}, 10)func run(str string) { defer wg.Done() //每当跑步时, 给channel写入一个数据, 这里struct{}{}是什么, 我们是不不关心的 //当channel元素到 10 个, 这里开始阻塞 batchCh &lt;- struct{}{} fmt.Printf(&quot;%v %s \\n&quot;, unix.Gettid(), str) Sleep(Second) //每跑完一个, channel 减一个元素 &lt;-batchCh}func TestParallel(t *testing.T) { for i := 0; i &lt; 10000; i++ { //t.Skip(&quot;Skipping testing&quot;) wg.Add(1) go run(&quot;Runner No.&quot; + strconv.Itoa(i) + &quot; finished&quot;) } wg.Wait()} 运行后, 我们可以看出, 此时, 每10个线程一批开始执行 总结我们这里实际还是利用了channel阻塞这一特性, 当然这里并不局限这一种方法","link":"/2022/03/05/go/basic/go-golang-limit-parallel-execution-md/"},{"title":"go中实现简单异步回调函数","text":"回调函数回调函数是指当程序执行到某种条件时, 执行一些action, 比如, 当程序正确执行完成, 回调某函数, 这里比较绕的地方就是, 此程序是异步执行, 主线程是不关心它是否结束, 所以也就产生了回调的概念 使用场景假设我们有一个程序A正在运行, 同时, 我们希望异步执行B程序, 执行完成后让用户知道, B执行完成, 比如打印什么的, 那么这种情况就是一种异步回调 代码我们模拟一个这样的场景, task线程比如我们就是一个耗时的操作sleep, 那么我们希望B正常执行完成后, 回调函数, 控制台输出成功 123456789101112131415161718192021222324252627282930313233343536373839404142434445//定义一个channel来记录task的执行结果var result = make(chan string)//模拟一个耗时的操作, 操作完成给channel 写入successfunc task(){ time.Sleep(time.Second * 3) result &lt;- &quot;success&quot;}//启用一个协程来执行task//这里注意 return 是不会被阻塞的func start() chan string{ go task() return result}//定义一个回调函数func callback() { fmt.Println(&quot;do something more&quot;)}func runTask() (interface{}, error){ //运行task start() //使用select 来定义场景, 假设result有只值则回调callback 方法 //若执行过久则超时 select{ case: c := &lt;-result: callback() return c, nil case: &lt;- time.After(time.Second * 5): return nil, fmt.Errorf(&quot;\\ntimeout&quot;) } }func TestTimeout(t *testing.T) { r, err := runTask() if err != nil { fmt.Println(err) return } fmt.Println(r) } 总结这里利用了channel 和select来实现 timeout的控制, 同时, 也利用此特性来实现回调,这里注意我们可以直接将回调函数放在内部, 例如: 12345func task(call func()) { time.Sleep(time.Second * 3) result &lt;- &quot;success&quot; call()} 这里只是抛砖引玉, 可以根据需求来调整回调的位置及条件","link":"/2022/03/06/go/basic/go-golang-timeout-md/"},{"title":"go struct的指针","text":"指针概述最早接触指针是大学学C的时候, 印象最深刻的就是说指针即地址, 随着工作经验的增加, 知道了,程序初始化对象时, (通常)会在内存中开辟空间来存放这个对象, 那在计算机处理这个对象时,必然需要知道这个对象所在的位置, 那么这个位置就是指针, 也就是你买了个房子, 住了进去, 那么为了让别人知道你家在哪, 那得有个地址. 再次理解假设我们有一个对象 User1, 在内存中有一片自己的值空间: 123456||||| User1 |||||| id 1 || name LaoWang ||||||||||||||||||-----------------| 0xc00000e038 | 那么它本身必然是有一个地址, 上面这个图就是表达了这个意思, 0xc00000e038 就是它的地址 代编程语言中理解指针Java中我们一般不需要理解指针, 因为java已经将其弱化, 或者可以理解为你在new的时候, 拿到的对象实际已经包含指针, 比如 User u = new User(), 此时u(对象的引用)中已经含有指针. 而golang, 我个人理解, 算是C, C++等语言的一种变体, 即保留了C/C++的一些特性又结合了面向对象语言中的一些特性, 而指针正是保留C/C++的一部分. go语言中的指针Hello world下面的代码初始化了一个变量a, 然后用变量p, 获得了其指针并保存 1234a:=10var p *intp=&amp;afmt.Printf(&quot;%p\\n&quot;, p) 结构体的指针工作中, 我们最常见的就是结构体, 类似java中的各种对象, 当写java代码时, 我们不需要指针, 但是当我们读go代码时, 往往我们看到, 对结构体操作时, 通常是其指针的操作. 接下来, 我们假设一种常见的情况, 我们有一个User对象, 然后我们想通过ChangeId方法来修改它的值, 那么分使用指针和不使用指针的情况: 先定义结构体和构造方法 1234567891011121314151617181920212223242526272829//先定义一个结构体type User struct { Id int Sex int Name string}//定义一个&quot;构造器&quot;//这个构造器返回一个没有非指针的&quot;对象&quot;func NewUser() User { return User{}}//定义一个&quot;构造器&quot;//这个构造器返回&quot;对象&quot;的指针func NewUserPtr() *User { return new(User)}//编写一个ChangeId方法, 方法参数为User类型func ChangeId(u User) { u.Id = 10}//编写一个ChangeIdByPtr方法, 方法参数为User类型的指针func ChangeIdByPtr(u *User) { //fmt.Printf(&quot;parameter user address %p %p %T %v\\n&quot;, &amp;u, u, u, *u) u.Id = 30} 不使用指针的情况 12345678//那么我们来使用它user := NewUser()//打印结果可以得到{0 0 }fmt.Println(user)//调用ChangeId方法ChangeId(user)//{Id:0 Sex:0 Name:}fmt.Printf(&quot;%+v\\n&quot;, user) 可以看出, 此时 Id 并没有变成10, 依旧是0 使用指针的情况 12345678//那么我们来使用它user := NewUserPtr()//打印结果可以得到&amp;{0 0 }fmt.Println(user)//调用ChangeIdByPtr方法ChangeIdByPtr(user)//{Id:10 Sex:0 Name:}fmt.Printf(&quot;%+v\\n&quot;, user) 修改成功 为什么指针可以?一句话解释就是, go是在传参时, 传入的是值拷贝. 也就是说我们传入对象user1时, 不论是ChangeId(user User)还是ChangeIdByPtr(user * User), 传入的user是值相同的不同对象, 具体说, 假设内存中user 对象是: 123456||||| user1 |||||| id 1 || name LaoWang ||||||||||||||||||-----------------| 0xc00000e038 | 那么此时ChangeId(user)中的user,由于go是值拷贝传递,则这个方法参数的user是 123456||||| user |||||| id 1 || name LaoWang ||||||||||||||||||-----------------| 0xc00000e099 | 所以你改的id实际改的是地址为 0xc00000e099 这里的id, 所以0xc00000e038的对象, 依然是1. 那么为什么ChangeIdByPtr(user * User) 可以修改成功呢? 因为此时传入的user指针是这个样子 12345||||| user |||||| 0xc00000e038 ||||||||||||||||||-----------------| 0xc00000e119 | 那么此时, 方法体内, user.id=10, 就表示0xc00000e119指向的0xc00000e038指向的id更换为10, 而0xc00000e038指向的就是我们想改的对象, 所以他是成功的 以上的问题, 你可以打印在执行方法前后的对象的值, 指针, 类型, 来查看其变化过程, 例如: 1fmt.Printf(&quot;parameter user address %p %p %T %v\\n&quot;, &amp;u, u, u, *u) 总结对于刚刚从面向对象转入go开发时, 建议直接全部使用指针, 然后慢慢根据开发经验的积累来改变自己对指针的使用","link":"/2022/03/02/go/basic/go-struct-pointer-basic-md/"},{"title":"Golang 解决错误 File is not goimports -ed (goimports)","text":"问题当我们引入golangci-lint, 然后检查代码格式, 检查时会产生如下问题 1File is not `goimports`-ed (goimports) 这个是因为我们的代码中部分 import 格式并不符合linting 的规范 解决方法 手动解决: 1234//安装goimportsgo get golang.org/x/tools/cmd/goimports//更新报错的文件goimports -w -local mit.com/adm &lt;path-to-file&gt; 使用golang插件 12//安装goimportsgo get golang.org/x/tools/cmd/goimports 接下来在报错的go文件窗口, 进入菜单 Tools | Go Tools 点击 Goimports file","link":"/2022/05/06/go/error/fix-file-is-not-goimports-md/"},{"title":"go中实现简单的 Object builder","text":"对象建造者在Java 中我们常用建造者模式来构建对象, 相较于传统构造器, 它更加灵活, 但是需要前期编码更多, 那么我们是否可以也在go中实现呢? Java中的对象builder 下面的例子展示了Java 中是如何实现一个User对象的Builder, 可以看出创建User对象只需要根据需要以.的形式来添加属性值即可 1234567891011121314151617181920212223242526272829303132public class User { private final String phone; private final String address; public static class UserBuilder { private String phone; private String address; public UserBuilder phone(String phone) { this.phone = phone; return this; } public UserBuilder address(String address) { this.address = address; return this; } public User build() { User user = new User(this); validateUserObject(user); return user; } } public static void main(String[] args) { User user1 = new User.UserBuilder() .phone(&quot;1234567&quot;) .address(&quot;Fake address 1234&quot;) .build(); }} Go中的对象builder具体我们直接看代码, 这里代码比较简单直接 1234567891011121314151617181920212223242526272829303132333435363738394041type Product struct { Id int Name string }//Product构造函数func NewProduct() *Product { return new(Product)}//定义Builder包含相同的属性type ProductBuilder struct { id int name string}//定义Builder对象func NewProductBuilder() *ProductBuilder { return &amp;ProductBuilder{}}//定义设置id属性方法//这里返回builder, 以便以&quot;.&quot; 的方式继续调用func (builder *ProductBuilder) Id(id int) *ProductBuilder { builder.id = id return builder}//定义设置name属性方法func (builder *ProductBuilder) Name(name string) *ProductBuilder { builder.name = name return builder}//build方法返回Product对象func (builder *ProductBuilder) build() *Product { prod := NewProduct() prod.Id = builder.id prod.Name = builder.name return prod} 测试 1234productBuilder := NewProductBuilder()build := productBuilder.Id(123).Name(&quot;Milk&quot;).build()//&amp;{123 Milk}fmt.Println(build) 小结这里是一个简单的建造者, 其实我们也可以考虑将这里的设置属性通用化, 类似with(fileName, value), 这样会更加灵活, 这里和标准的建造者模式还是不一样的,因为建造者模式实际是以接口的形式, 泛化创建者, 比如上面的Product, 可以是ChineseProduct, JapaneseProduct, 然后定义Builder接口, 两种product 各有各的builder, 最后按需通过director 来调用.","link":"/2022/03/03/go/pattern/go-golang-object-builder-md/"},{"title":"[intellij] intellj无鼠标常用操作","text":"前言这些操作前提是你没自己改过默认设置, 我都是在linux ubuntu 下使用, windows和macOS, 会略有不同. 这些东西就是用多了就熟练了, 没什么技巧 操作1: 查看最近操作的文件查看最近操作的文件列表 1Ctrl+E 查看最近操作的文件及内容 1Ctrl+Shift+E 操作2: 声明方法之间跳转1Alt+Down 或 Up 操作3: 折叠方法体1Ctrl+Shift+Numpad 操作4: 上下移动方法体或行1Ctrl+Shift+Down 或 Up 操作5: 在文件之间移动1Ctrl+Right 或 Ctrl+Left 操作6: 打开终端1Alt+F12 操作7: 快速创建测试体1Alt+Insert","link":"/2021/10/26/idea/intellij/intellij-no-mouse-tips-1/"},{"title":"[Spring][Webflux]如何找出webflux中的阻塞方法","text":"0. 为什么需要找到Blocking call我们使用reactor编程时，其目的就是希望我们的程序符合异步非阻塞的模型，为了达到这个目的，我们希望我们程序中所有的方法都是非阻塞的方法(理想状态)，比如我们在处理JDBC链接时，会考虑使用Schedulers来包裹或是使用R2DBC，那么在响应式编程中，我们会遇到形形色色的阻塞方法，此时，我们就需要用合理的方式处理它们了. 1. 解决方案BlockHound 2. Git 地址https://github.com/reactor/BlockHound 3. 大致原理类似于Java代理，再入口函数调用前被JVM加载，一旦BlockHound启动，其将标记阻塞方法(例如: sleep()) .并改变其behaviour而抛出一个Error 4. 引入BlockHound在自己的工程中引入BlockHound 4.1. maven1234567&lt;dependency&gt; &lt;groupId&gt;io.projectreactor.tools&lt;/groupId&gt; &lt;artifactId&gt;blockhound-junit-platform&lt;/artifactId&gt; &lt;version&gt;1.0.0.RC1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 4.2. Gradle123456789101112repositories {mavenCentral()// maven { url 'https://repo.spring.io/milestone' }// maven { url 'https://repo.spring.io/snapshot' }}dependencies {testCompile 'io.projectreactor.tools:blockhound:$LATEST_RELEASE'// testCompile 'io.projectreactor.tools:blockhound:$LATEST_MILESTONE'// testCompile 'io.projectreactor.tools:blockhound:$LATEST_SNAPSHOT'} 5. 使用示例1234567891011121314151617181920212223242526272829public class DetectBlockingCall { @BeforeEach void setUp() { // 1. 初始化BlockHound BlockHound.install(); } // 2. 定义一个阻塞方法 void blockingCall() { Mono.delay(Duration.ofSeconds(1)) .doOnNext(it -&gt; { try { Thread.sleep(10); } catch (InterruptedException e) { throw new RuntimeException(e); } }) .block(); } @Test void blockHoundSimpleTest() { //3. 调用阻塞方法 Throwable throwable = Assertions.assertThrows(Throwable.class, this::blockingCall); //4. 验证阻塞方法是否抛出异常 Assertions.assertTrue(throwable.getMessage().contains(&quot;Blocking call!&quot;)); }} 在这个示例中，第一步加载BlockHound实际是可以省略的，因为我们引入BlockHound到junit 实际是已经被预加载, 大家可以去除这一步再次执行测试代码尝试 6. 构建项目时自动执行BlockHound往往我们希望我们自身的项目可以自动执行BlockHound，从而每次运行测试代码便可以知道我们的代码问题在哪里，那么这里提供一种思路，即使用项目构建工具来执行BlockHound, 以Gradle为例. 6.1. 编写定制化BlockHound模块 (当然你可以不定制化)在开发中，往往我们不可避免的使用部分部分阻塞方法，那么此时我们需要测试时排除这些方法. 此时我们可以定义一些定制化类，例如: 新建一个工程com.test.support， 新建一个模块叫做blockhound-integration, 然后新建一个Log的忽略类 123456789101112131415public class LogBlockHoundIntegration implements BlockHoundIntegration {// 使用系统变量来达到开关的目的 private static final boolean ENABLED = Boolean.parseBoolean(System.getProperty(&quot;LogBlockHoundIntegration.enabled&quot;, Boolean.FALSE.toString())); @Override public void applyTo(BlockHound.Builder builder) { if (!ENABLED) { return; } // 加入要忽略的阻塞方法builder.allowBlockingCallsInside( &quot;ch.qos.logback.classic.Logger&quot;, &quot;buildLoggingEventAndAppend&quot;); }} 6.2. 定义测试监听类实现TestExecutionListener, 静态加载BlockHound，使得所有测试方法都需要加载BlockHound 123456789101112public class BlockHoundTestExecutionListener implements TestExecutionListener { static { BlockHound.install(builder -&gt; { builder.blockingMethodCallback(method -&gt; { Error error = new BlockingOperationError(method); error.printStackTrace(System.err); throw error; }); }); }} 6.3. 在自己模块的gradle文件中定义方法，引入我们的定义及默认的junit平台12345678ext { // add helper to activate Reactor BlockHound, https://github.com/reactor/BlockHound useReactorBlockHound = { -&gt; project.dependencies { testRuntimeOnly 'com.test.support:blockhound-integration', 'org.junit.platform:junit-platform-launcher' } }} 6.4. 定义执行操作入口build.gradle 中插入 1234567891011subprojects { subproject -&gt; subproject.useReactorBlockHound()}// 打开我们自己定义的生效类tasks.withType(Test) { // ignore the blocking nature of Log systemProperty 'LogBlockHoundIntegration.enabled', 'true' } 至此，我们基本可以满足gradle项目开发中所需要的自动化测试了。如果你在使用maven，可以构建自己的maven插件，来实现自动化流程，具体逻辑与gradle是类似的 6. 结论响应式编程是基于我们想充分利用异步非阻塞而产生的一种设计，但如今我理解技术正处于一个转型期，往往我们会遇到阻塞+非阻塞的囧境，为了解决这个问题，今天引入BlockHound工具来探测我们程序中潜在的阻塞API，使我们更快的发现问题并做出调整. 7. refhttps://medium.com/@domenicosibilio/blockhound-detect-blocking-calls-in-reactive-code-before-its-too-late-6472f8ad50c1","link":"/2021/07/02/java/spring/%E6%89%BE%E5%87%BAwebflux%E4%B8%AD%E7%9A%84%E9%98%BB%E5%A1%9E%E6%96%B9%E6%B3%95/"},{"title":"如何查看容器对应的宿主上打开的文件","text":"查看容器对应的pid查看容器id1234$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b71e02952ed1 postgres:13 &quot;docker-entrypoint.s…&quot; 2 weeks ago Up 2 weeks 0.0.0.0:5432-&gt;5432/tcp docker_db_1 查看容器对应的宿主pid 通过 docker container top12345678910$ docker container top b71e02952ed1 UID PID PPID C STIME TTY TIME CMD 999 20969 20950 0 tammi19 ? 00:00:28 postgres 999 21228 20969 0 tammi19 ? 00:00:00 postgres: checkpointer 999 21229 20969 0 tammi19 ? 00:00:16 postgres: background writer 999 21230 20969 0 tammi19 ? 00:00:16 postgres: walwriter 999 21231 20969 0 tammi19 ? 00:00:23 postgres: autovacuum launcher 999 21233 20969 0 tammi19 ? 00:00:47 postgres: stats collector 999 21234 20969 0 tammi19 ? 00:00:00 postgres: logical replication launcher 使用直接访问容器对应的cgroup 文件夹 /sys/fs/cgroup/memory/docker/ 的方式查看 12345678910111213#看到b71e02952ed1 这个容器文件夹$ls /sys/fs/cgroup/memory/docker/ b71e02952ed16769ffdfbf67dfbec8ed622bdc934524a6547f4197b9657ca623 memory.kmem.limit_in_bytes memory.kmem.usage_in_bytes #同样可以看到其对应的pid$cat /sys/fs/cgroup/memory/docker/b71e02952ed16769ffdfbf67dfbec8ed622bdc934524a6547f4197b9657ca623/cgroup.procs 20969 21228 21229 21230 21231 21233 21234 查看pid 对应的打开文件 使用 lsof -p 12345678910$ sudo lsof -p 20950 ... ...COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME container 20950 root cwd DIR 0,25 120 1356 /run/containerd/io.containerd.runtime.v1.linux/moby/b71e02952ed16769ffdfbf67dfbec8ed622bdc934524a6547f4197b9657ca623 container 20950 root rtd DIR 253,1 4096 2 / container 20950 root txt REG 253,1 6117096 27010813 /usr/bin/containerd-shim container 20950 root 0r CHR 1,3 0t0 6 /dev/null container 20950 root 1w CHR 1,3 0t0 6 /dev/null container 20950 root 2w CHR 1,3 0t0 6 /dev/null ... ... 通过查看程文件夹 /proc//fd 123456789101112131415$sudo ls -l /proc/20950/fd total 0 lr-x------ 1 root root 64 Feb 3 09:47 0 -&gt; /dev/null l-wx------ 1 root root 64 Feb 3 09:47 1 -&gt; /dev/null lrwx------ 1 root root 64 Feb 3 09:47 10 -&gt; 'socket:[4029583985]' lr-x------ 1 root root 64 Feb 3 09:47 11 -&gt; 'pipe:[4029582852]' l-wx------ 1 root root 64 Feb 3 09:47 12 -&gt; /run/docker/containerd/b71e02952ed16769ffdfbf67dfbec8ed622bdc934524a6547f4197b9657ca623/init-stdout lr-x------ 1 root root 64 Feb 3 09:47 13 -&gt; 'pipe:[4029582853]' l--------- 1 root root 64 Feb 3 09:47 14 -&gt; /run/docker/containerd/b71e02952ed16769ffdfbf67dfbec8ed622bdc934524a6547f4197b9657ca623/init-stdout l--------- 1 root root 64 Feb 3 09:47 15 -&gt; /run/docker/containerd/b71e02952ed16769ffdfbf67dfbec8ed622bdc934524a6547f4197b9657ca623/init-stdout lr-x------ 1 root root 64 Feb 3 09:47 16 -&gt; /run/docker/containerd/b71e02952ed16769ffdfbf67dfbec8ed622bdc934524a6547f4197b9657ca623/init-stdout l--------- 1 root root 64 Feb 3 09:47 17 -&gt; /run/docker/containerd/b71e02952ed16769ffdfbf67dfbec8ed622bdc934524a6547f4197b9657ca623/init-stderr l-wx------ 1 root root 64 Feb 3 09:47 18 -&gt; /run/docker/containerd/b71e02952ed16769ffdfbf67dfbec8ed622bdc934524a6547f4197b9657ca623/init-stderr l--------- 1 root root 64 Feb 3 09:47 19 -&gt; /run/docker/containerd/b71e02952ed16769ffdfbf67dfbec8ed622bdc934524a6547f4197b9657ca623/init-stderr l-wx------ 1 root root 64 Feb 3 09:47 2 -&gt; /dev/null","link":"/2022/02/03/linux/command/linux-command-how-to-check-container-opened-file-md/"},{"title":"lsof的权限问题","text":"lsof 了解linux 一切皆文件, lsof – lists open files, 所以, 这个命令理论上, 你可以看到所有打开的文件, 也就是所有打开的东西, 比如, 文本, 网络链接, 进程, 等 常见用法不是本文讨论重点, 可以自行谷歌常见用法 实验一般教程往往给定了root权限, 所以当使用lsof时, 可以看到所有文件信息, 但是生产环境不一定, 这里我们做一个实验, 创建一个普通用户, 对比另一个用户执行lsof 创建一个用户 12sudo useradd testersudo passwd tester 模拟一个进程来占用文件 12345touch test.txttail -f test.txt#为了方便观察, 先删了这个文件, 注意, 此时tail 还在访问这个文件rm test.txt 然后lsof 查看 123dev@vm:~&gt; sudo lsof -l|grep deleted... ...tail 20598 1001 3r REG 253,3 0 1663794 /home/dev/test.txt (deleted) 转换成刚刚创建的用户再次查看 12345eccd@worker-pool1-al0h1t9p-efggjjp-ibd-test:~&gt; su - testerPassword: tester@vm&gt; lsof -l|grep deleted#没有同样的输出 结论lsof 实际是和权限有关的, 这个其实也是很显而易见的, 往往有时忽略权限问题, 导致结果就是排查问题会忘记权限影响的输出结果","link":"/2022/02/03/linux/command/lsof-permission-study-md/"},{"title":"Makefile入门","text":"Makefile 是什么不写具体定义, 个人理解就是一个通用的将代码组织起来的工具, 在java中我们常用maven或者gradle, 他们可以帮助我们编译java, 打包, 配置等等. 而makefile则是将这个过程交给开发人员本身, 用目标文件和它是怎么来的定义make的过程, 比如, 我们有一个, 可执行文件a, 这个a引用了b, 那么我们定义这个a的依赖过程, 就可以写在make中 举个栗子 入门例子1 - hello此时,创建一个Makefile文件(建议习惯性使用Makefile作为文件名, 避免不同系统直接拷贝后不可用的问题), 写入下面内容(注意, 第二行开头是tab键不是空格), 执行 make hello 12hello: @echo &quot;hello world&quot; 会输出 “hello world”, 这里的@符号是为了除去输出echo 语句 例子2 - 依赖传递下面这个例子中, 我们定义一个main函数, 这个函数引用, add.c 和 dev.c. 然后在main函数中调用add和dev中的方法. 这里我们就需要用到 编译后的add和dev文件 add.o及dev.o给main.c: 首先定义两个函数(以下代码放到同一个文件夹下), add 和 dev,这里重点关注make, 两个函数只是随意定义的即可 add 头文件 add.h 1234#ifndef UNTITLED1_ADD_H#define UNTITLED1_ADD_Hint add(int x, int y);#endif //UNTITLED1_ADD_H add.c 123int add(int x, int y) { return x + y;} dev 头文件 dev.h 1234#ifndef UNTITLED1_DEV_H#define UNTITLED1_DEV_Hint dev(int i, int j);#endif //UNTITLED1_DEV_H dev.c 123int dev(int x, int y) { return x / y;} main.c 这里main引用了add 和 dev 12345678910#include &lt;stdio.h&gt;#include &quot;add.h&quot;#include &quot;dev.h&quot;int main() { printf(&quot;Hello, World!\\n&quot;); printf(&quot;add %i!\\n&quot;, add(1,2)); printf(&quot;dev %i!\\n&quot;, dev(2,1)); return 0;} Makefile 这里注意 $^ 表示所有依赖, $@ 表示所有输出, 比如第一行的$^表示add.c, 也可以写成 gcc -c add.c 比如第一个目标文件,是add.o 第三个目标是main文件, 它的依赖则是 main.c, add.o, dev.o12345678add.o:add.c gcc -c $^dev.o:dev.c gcc -c $^main:main.c add.o dev.o gcc $^ -o $@ 运行 make man, 可以即得到我们预期的main 的二进制文件, 使用./main执行即可. 如果此时你查看当前目录, 也可以看到add.o, dev.o 文件, 因为这些是中间目标文件. 删除目标文件执行完或者执行前, 我们希望删除之前生成的文件 123456789101112#定义一个变量来写clean C_CLEAN=clean# 定义clean 目标$(C_CLEAN): rm *.o main# 定义一个all 来每次执行&quot;main&quot; 前都先删除之前的目标文件 all: $(C_CLEAN) main# 定义伪目标.PHONY: $(C_CLEAN) 那么, 此时, 我们只需要执行make all 即可. 这里定义了一个伪目标, 避免当存在文件名为clean文件时, 因为makefile是以文件为默认目标, 且先尝试找文件, 编译失败, 具体大家可以搜搜.PHONY的详细用法 所有代码目录 1234567891011.├── add.c├── add.h├── cmake-build-debug├── CMakeLists.txt├── dev.c├── dev.h├── main.c└── Makefile1 directory, 7 files Makefile 12345678910111213141516171819202122232425262728293031323334353637A=helloB=worldC=big $(B)C_SOURCE=main.c dev.c add.cC_CLEAN=clean#目标文件一定要存在, 依赖文件可以不需要#echo &quot;hello world&quot;#hello world#添加@后, 不会打印#hello worldhello: @echo &quot;hello world&quot;add.o:add.c gcc -c $^dev.o:dev.c gcc -c $^main:main.c add.o dev.o gcc $^ -o $@all: $(C_CLEAN) main#使用变量vars: @echo $A @echo $B @echo $C$(C_CLEAN): rm *.o main.PHONY: $(C_CLEAN) Github代码 总结单纯Makefile是非常强大的, 因为它把项目的编译主动权交给了开发人员, 那么个人理解, 如果是一些Iaas和Paas层的开发较常见使用, 比如涉及容器, kubernetes, vm等. 而软件层面往往使用现有的封装好的框架工具比如gradle此类即可.","link":"/2022/02/03/linux/command/makefile-basic-md/"},{"title":"记一次kubelet假死的问题排查","text":"发现问题今天在升级kubernetes集群时，被升级节点突然开始变得异常缓慢，ssh需要很久很久, 但是却可以ping的通. 有问题如下: 节点缓慢 可以ping 不可以ssh 健康节点上 kubectl describe node, 可以看到 123456789Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- NetworkUnavailable False Wed, 28 Sep 2022 07:08:34 +0000 Wed, 28 Sep 2022 07:08:34 +0000 CalicoIsUp Calico is running on this node MemoryPressure Unknown Wed, 28 Sep 2022 07:46:33 +0000 Wed, 28 Sep 2022 07:47:36 +0000 NodeStatusUnknown Kubelet stopped posting node status. DiskPressure Unknown Wed, 28 Sep 2022 07:46:33 +0000 Wed, 28 Sep 2022 07:47:36 +0000 NodeStatusUnknown Kubelet stopped posting node status. PIDPressure Unknown Wed, 28 Sep 2022 07:46:33 +0000 Wed, 28 Sep 2022 07:47:36 +0000 NodeStatusUnknown Kubelet stopped posting node status. Ready Unknown Wed, 28 Sep 2022 07:46:33 +0000 Wed, 28 Sep 2022 07:47:36 +0000 NodeStatusUnknown Kubelet stopped posting node status.Addresses: 排查问题 ssh 到问题节点, 拿到shell并root 在describe node 时， 看到kubelet有问题， 那么先查看kubelet 首先查看kubelet 状态 123456789101112131415161718192021222324journalctl -u kubelet|lessorroot@node-15-cp-gswpw:~&gt; systemctl status kubelet● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/local/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2022-09-28 06:39:55 UTC; 1h 14min ago Docs: http://kubernetes.io/docs/ Main PID: 8440 (kubelet) Tasks: 27 CGroup: /system.slice/kubelet.service └─8440 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --read-only-port=0 --config=/var/lib/kubelet/config.yaml --&gt;Sep 28 07:54:18 node-15-cp-gswpw.novalocal kubelet[8440]: E0928 07:54:18.267169 8440 kubelet_volumes.go:245] &quot;There were many similar errors. Turn up verbosity to see them.&quot; err=&quot;orphaned pod \\&quot;7181c&gt;Sep 28 07:54:20 node-15-cp-gswpw.novalocal kubelet[8440]: E0928 07:54:20.268365 8440 kubelet_volumes.go:245] &quot;There were many similar errors. Turn up verbosity to see them.&quot; err=&quot;orphaned pod \\&quot;7181c&gt;Sep 28 07:54:22 node-15-cp-gswpw.novalocal kubelet[8440]: E0928 07:54:22.268410 8440 kubelet_volumes.go:245] &quot;There were many similar errors. Turn up verbosity to see them.&quot; err=&quot;orphaned pod \\&quot;7181c&gt;Sep 28 07:54:24 node-15-cp-gswpw.novalocal kubelet[8440]: E0928 07:54:24.266872 8440 kubelet_volumes.go:245] &quot;There were many similar errors. Turn up verbosity to see them.&quot; err=&quot;orphaned pod \\&quot;7181c&gt;Sep 28 07:54:26 node-15-cp-gswpw.novalocal kubelet[8440]: E0928 07:54:26.268006 8440 kubelet_volumes.go:245] &quot;There were many similar errors. Turn up verbosity to see them.&quot; err=&quot;orphaned pod \\&quot;7181c&gt;Sep 28 07:54:28 node-15-cp-gswpw.novalocal kubelet[8440]: E0928 07:54:28.269099 8440 kubelet_volumes.go:245] &quot;There were many similar errors. Turn up verbosity to see them.&quot; err=&quot;orphaned pod \\&quot;7181c&gt;Sep 28 07:54:30 node-15-cp-gswpw.novalocal kubelet[8440]: E0928 07:54:30.266409 8440 kubelet_volumes.go:245] &quot;There were many similar errors. Turn up verbosity to see them.&quot; err=&quot;orphaned pod \\&quot;7181c&gt;Sep 28 07:54:32 node-15-cp-gswpw.novalocal kubelet[8440]: E0928 07:54:32.268111 8440 kubelet_volumes.go:245] &quot;There were many similar errors. Turn up verbosity to see them.&quot; err=&quot;orphaned pod \\&quot;7181c&gt;Sep 28 07:54:34 node-15-cp-gswpw.novalocal kubelet[8440]: E0928 07:54:34.266758 8440 kubelet_volumes.go:245] &quot;There were many similar errors. Turn up verbosity to see them.&quot; err=&quot;orphaned pod \\&quot;7181c&gt;Sep 28 07:54:36 node-15-cp-gswpw.novalocal kubelet[8440]: E0928 07:54:36.272904 8440 kubelet_volumes.go:245] &quot;There were many similar errors. Turn up verbosity to see them.&quot; err=&quot;orphaned pod \\&quot;7181 看到kubelet 实际是启动的，但是有报错 查看api server 1crictl ps -a 看到 api server 是退出的状态, 在查看它的日志 1crictl log 87d3992f84f74 看到许多的链接异常, 并看到etcd异常, 此时基本断定，etcd通讯异常和我们ssh应该是一个问题，也就是说不是kubernetes 的问题了 排查系统层面的问题 查看进程 top 后， 看到 kswapd0 占用高cpu, 简单说， 就是系统现在没内存了， 要异步回收资源, 看到这里就已经有点问题的苗头了.而且 load average 值过高(实际要观察其1-15分钟的变化)， 现在很多进程需要处理. 12345678910111213141516171819202122232425262728293031323334toptop - 08:20:09 up 1:43, 1 user, load average: 176.16, 177.88, 186.03Tasks: 373 total, 53 running, 316 sleeping, 0 stopped, 4 zombie%Cpu(s): 0.2 us, 99.8 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.1 si, 0.0 stMiB Mem : 3921.832 total, 86.609 free, 3869.438 used, 210.109 buff/cacheMiB Swap: 0.000 total, 0.000 free, 0.000 used. 52.395 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 108 root 20 0 0 0 0 R 69.09 0.000 38:38.54 kswapd0 8690 root 15 -5 1801280 828812 0 S 35.76 20.64 28:20.95 kube-apiserver 5561 root 20 0 819188 19556 0 S 22.12 0.487 11:00.48 kube-controller 116886 292100 20 0 4240176 280800 0 S 17.27 6.992 3:00.43 java 5579 root 20 0 752904 26280 0 R 16.97 0.654 9:51.93 kube-scheduler 116503 211908 20 0 4232144 280032 0 S 16.06 6.973 3:04.31 java 7799 root 15 -5 9.892g 128688 0 R 15.45 3.204 13:20.12 etcd 100063 root 20 0 751580 20368 0 R 14.85 0.507 3:54.08 coredns 115499 root 20 0 1454732 88380 440 S 14.24 2.201 2:50.97 vmagent 69120 root 20 0 2042904 21748 0 S 9.697 0.542 13:06.08 calico-node 69121 root 20 0 1894928 15780 0 S 9.394 0.393 3:28.76 calico-node 102022 root 20 0 1798948 227396 0 S 9.091 5.662 2:53.44 vmstorage 35393 root 20 0 728996 20380 0 R 8.485 0.507 4:40.08 registry 101284 288256 20 0 1260044 12480 0 R 8.182 0.311 1:54.41 controller 8440 root 20 0 2817068 98476 0 S 7.879 2.452 10:45.11 kubelet 119482 root 20 0 12292 1108 900 R 7.576 0.028 0:00.49 sh 117868 root 20 0 118232 7604 6472 R 7.273 0.189 1:08.98 systemd-journal 69118 root 20 0 1452024 12740 0 R 6.667 0.317 1:33.67 calico-node 26767 root 20 0 965176 294476 30348 R 6.364 7.333 5:27.58 falco 113762 root 20 0 1164648 13464 0 S 6.364 0.335 1:29.39 vmselect 115675 128052 20 0 1334232 10372 0 R 6.364 0.258 1:36.60 app 10800 root 20 0 712264 8672 0 S 6.061 0.216 1:02.54 xx-local-execu 114380 root 20 0 1231764 67000 0 S 5.758 1.668 1:19.32 vminsert 119450 root 20 0 724712 528 0 R 5.455 0.013 0:01.48 crictl 119456 root 20 0 45896 2956 1892 R 5.455 0.074 0:00.77 top 查看内核日志 为了确定这一点， 来查看内核日志. 需要注意的是， 内核日志有可能刷新太快， 你grep 时不一定有输出， 要持续观察 123-bash-4.4# grep -i &quot;Out Of Memory&quot; /var/log/messages2022-09-28T08:24:19.537448+00:00 node-15-cp-gswpw kernel: [ 6472.442368] Out of memory: Killed process 101788 (python3) total-vm:487148kB, anon-rss:54264kB, file-rss:0kB, shmem-rss:0kB2022-09-28T08:27:28.284397+00:00 node-15-cp-gswpw kernel: [ 6661.390988] Out of memory: Killed process 113716 (python3) total-vm:297436kB, anon-rss:49260kB, file-rss:0kB, shmem-rss:0kB 看到上面的信息， 我们也就看到了问题, 就是 OOM 查看僵尸进程 hmmm… 这么多 (Z)Zombie 状态的进程，很多都是和网络有关,这就是什么所有链接的怪怪的 123456789101112-bash-4.4# ps aux|grep ZUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 101788 0.2 0.0 0 0 ? Zs 07:50 0:06 [python3] &lt;defunct&gt;root 113716 0.2 0.0 0 0 ? Zs 07:55 0:05 [python3] &lt;defunct&gt;root 115424 0.2 0.0 0 0 ? Zsl 07:55 0:05 [python3] &lt;defunct&gt;128052 117448 0.0 0.0 0 0 ? Z 07:55 0:00 [curl] &lt;defunct&gt;root 118982 0.0 0.0 0 0 ? Z 08:17 0:00 [ip6tables] &lt;defunct&gt;root 118994 0.0 0.0 0 0 ? Z 08:17 0:00 [iptables] &lt;defunct&gt;128052 119206 0.4 0.0 0 0 ? Z 08:18 0:03 [curl] &lt;defunct&gt;root 119324 1.7 0.0 0 0 ? Zs 08:18 0:12 [calico-node] &lt;defunct&gt;root 119460 0.2 0.0 0 0 ? Z 08:19 0:01 [iptables] &lt;defunct&gt;root 119736 16.4 0.0 10248 1512 pts/1 S+ 08:30 0:00 grep Z 查看当前系统的内存 只有3个G， free 是0. (注意free 是0 不能断定系统没内存了, 详情 ref： https://www.linuxatemyram.com/) 1234-bash-4.4# free -g total used free shared buff/cache availableMem: 3 3 0 0 0 0Swap: 0 0 0 多一嘴 OOM Killer 会在系统内存相当吃紧时被调用， 此时， 很多进程就会被杀死， 那么整个系统就并不可靠了， 因为你不知道谁被杀了 解决问题加内存， 或者将一些应用的内存调小","link":"/2022/09/28/kubernetes/debug_kubelet_dead_oom/"}],"tags":[{"name":"ansible","slug":"ansible","link":"/tags/ansible/"},{"name":"centos8","slug":"centos8","link":"/tags/centos8/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"kubernetes","slug":"kubernetes","link":"/tags/kubernetes/"},{"name":"kubernetes工具","slug":"kubernetes工具","link":"/tags/kubernetes%E5%B7%A5%E5%85%B7/"},{"name":"network","slug":"network","link":"/tags/network/"},{"name":"openstack","slug":"openstack","link":"/tags/openstack/"},{"name":"openstack常用操作","slug":"openstack常用操作","link":"/tags/openstack%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"},{"name":"runc","slug":"runc","link":"/tags/runc/"},{"name":"vagrant","slug":"vagrant","link":"/tags/vagrant/"},{"name":"tutorial","slug":"tutorial","link":"/tags/tutorial/"},{"name":"hugo","slug":"hugo","link":"/tags/hugo/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"jenkins","slug":"jenkins","link":"/tags/jenkins/"},{"name":"intellij","slug":"intellij","link":"/tags/intellij/"},{"name":"pipeline","slug":"pipeline","link":"/tags/pipeline/"},{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"thread","slug":"thread","link":"/tags/thread/"},{"name":"channel","slug":"channel","link":"/tags/channel/"},{"name":"pointer","slug":"pointer","link":"/tags/pointer/"},{"name":"error","slug":"error","link":"/tags/error/"},{"name":"builder","slug":"builder","link":"/tags/builder/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"webflux","slug":"webflux","link":"/tags/webflux/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"lsof","slug":"lsof","link":"/tags/lsof/"},{"name":"makefile","slug":"makefile","link":"/tags/makefile/"}],"categories":[{"name":"ansible","slug":"ansible","link":"/categories/ansible/"},{"name":"git","slug":"git","link":"/categories/git/"},{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"kubernetes","slug":"kubernetes","link":"/categories/kubernetes/"},{"name":"openstack","slug":"openstack","link":"/categories/openstack/"},{"name":"runc","slug":"runc","link":"/categories/runc/"},{"name":"vagrant","slug":"vagrant","link":"/categories/vagrant/"},{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"jenkins","slug":"jenkins","link":"/categories/jenkins/"},{"name":"golang","slug":"golang","link":"/categories/golang/"},{"name":"network","slug":"kubernetes/network","link":"/categories/kubernetes/network/"},{"name":"intellij","slug":"intellij","link":"/categories/intellij/"},{"name":"java","slug":"java","link":"/categories/java/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"spring","slug":"java/spring","link":"/categories/java/spring/"},{"name":"webflux","slug":"java/spring/webflux","link":"/categories/java/spring/webflux/"}],"pages":[{"title":"关于我","text":"I’m a Daddy who code in different areas, I hope my blog could give you some ideas Je suis un Papa qui program dans les domaines différents, J’espère que mon site vous aide un peu Olen isä, olen kehittäjä 爸爸, 丈夫, 程序员","link":"/about/index.html"},{"title":"copyright","text":"基本信息 作者: Jiang WU 权利人: Jiang WU 联系方式: johnwufr@gmail.com 版权声明 本版权声明承袭著作权法, 在此基础上额外声明以下规定. 本网站文章, 帖子等仅代表作者本人的观点, 本站不保证文章等内容的有效性. 属在本网站发表的文章（包括转帖）, 版权归原作者所有. 本网站会不定期的对本站的著作进行审查, 对于收录的文章, 会首先征求作者同意 本网站部分内容转载于合作站点或其他站点,但都会注明作/译者和原出处. 如有不妥之处,敬请指出 在征得本网站,以及作, 译者同意的情况下,本网站的作品允许非盈利性引用,于本站原创作品并请注明出处：”作者：Jiang WU 转载自 unprobug.com”字样, 于非本站原创作品请联系创作者版权问题, 以尊重作者的劳动成. 版权归原作/译者所. 未经允许,严禁转载. 对非法转载者,其行为包括未经允许的引用, 转载, 复制的片段, 改编, 演绎, 二次转载, 本站和作/译者保留采用法律手段追究的权利. 授权许可 非商业用途, 默认许可, 但需遵循以上版权声明 商业用途, 需要征得本网站许可, 并获得本网站签发的许可证书, 方可获得授权 商业洽谈 请通过邮箱联系本站管理员 免责声明 本站不负责在本站留言中的恶意诋毁, 污蔑, 及网络攻击等违法行为造成的后果","link":"/copyright/index.html"}]}